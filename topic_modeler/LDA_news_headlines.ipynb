{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('title_abstract.csv', error_bad_lines=False);\n",
    "data_text = data[['text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3982"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A strategy for managing content complexity in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficient passage ranking for document databas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The aditi deductive database system:Deductive ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Housekeeping for prefix coding:We consider the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Memory efficient ranking:Fast and effective ra...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  index\n",
       "0  A strategy for managing content complexity in ...      0\n",
       "1  Efficient passage ranking for document databas...      1\n",
       "2  The aditi deductive database system:Deductive ...      2\n",
       "3  Housekeeping for prefix coding:We consider the...      3\n",
       "4  Memory efficient ranking:Fast and effective ra...      4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Using', 'emerging', 'patterns', 'and', 'decision', 'trees', 'in', 'rare-class', 'classification:The', 'problem', 'of', 'classifying', 'rarely', 'occurring', 'cases', 'is', 'faced', 'in', 'many', 'real', 'life', 'applications.', 'The', 'scarcity', 'of', 'the', 'rare', 'cases', 'makes', 'it', 'difficult', 'to', 'classify', 'them', 'correctly', 'using', 'traditional', 'classifiers.', 'In', 'this', 'paper,', 'we', 'propose', 'a', 'new', 'approach', 'to', 'use', 'emerging', 'patterns', '(EPs)', 'and', 'decision', 'trees', '(DTs)', 'in', 'rare-class', 'classification', '(EPDT).', 'EPs', 'are', 'those', 'itemsets', 'whose', 'supports', 'in', 'one', 'class', 'are', 'significantly', 'higher', 'than', 'their', 'supports', 'in', 'the', 'other', 'classes.', 'EPDT', 'employs', 'the', 'power', 'of', 'EPs', 'to', 'improve', 'the', 'quality', 'of', 'rare-case', 'classification.', 'To', 'achieve', 'this', 'aim,', 'we', 'first', 'introduce', 'the', 'idea', 'of', 'generating', 'new', 'non-existing', 'rare-class', 'instances,', 'and', 'then', 'we', 'over-sample', 'the', 'most', 'important', 'rare-class', 'instances.', 'Our', 'experiments', 'show', 'that', 'EPDT', 'outperforms', 'many', 'classification', 'methods.', '©', '2004', 'IEEE.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['emerge', 'pattern', 'decision', 'tree', 'rare', 'class', 'classification', 'problem', 'classify', 'rarely', 'occur', 'case', 'face', 'real', 'life', 'applications', 'scarcity', 'rare', 'case', 'make', 'difficult', 'classify', 'correctly', 'traditional', 'classifiers', 'paper', 'propose', 'approach', 'emerge', 'pattern', 'decision', 'tree', 'rare', 'class', 'classification', 'epdt', 'itemsets', 'support', 'class', 'significantly', 'higher', 'support', 'class', 'epdt', 'employ', 'power', 'improve', 'quality', 'rare', 'case', 'classification', 'achieve', 'introduce', 'idea', 'generate', 'exist', 'rare', 'class', 'instance', 'sample', 'important', 'rare', 'class', 'instance', 'experiment', 'epdt', 'outperform', 'classification', 'methods', 'ieee']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 200].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    [efficient, consumer, response, survey, austra...\n",
       "11    [binary, interpolative, cod, effective, index,...\n",
       "12    [empirical, evaluation, cod, methods, multi, s...\n",
       "13    [fast, algorithm, meld, splay, tree, springer,...\n",
       "14    [efficient, object, orient, program, prolog, d...\n",
       "15    [optimal, dynamic, multi, attribute, hash, ran...\n",
       "16    [determinism, functional, languages, introduct...\n",
       "17    [efficient, computation, query, stratify, data...\n",
       "18    [share, groundness, dependencies, logic, progr...\n",
       "19    [linear, arboricity, linear, arboricity, regul...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 accompani\n",
      "1 action\n",
      "2 address\n",
      "3 advantag\n",
      "4 algorithm\n",
      "5 allow\n",
      "6 anim\n",
      "7 avail\n",
      "8 call\n",
      "9 captur\n",
      "10 complex\n",
      "11 content\n",
      "12 control\n",
      "13 coordin\n",
      "14 correspond\n",
      "15 data\n",
      "16 descript\n",
      "17 detail\n",
      "18 differ\n",
      "19 dofferem\n",
      "20 dynam\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 5),\n",
       " (5, 1),\n",
       " (6, 4),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 3),\n",
       " (11, 2),\n",
       " (12, 2),\n",
       " (13, 1),\n",
       " (14, 2),\n",
       " (15, 2),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 2),\n",
       " (24, 2),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 2),\n",
       " (30, 2),\n",
       " (31, 1),\n",
       " (32, 5),\n",
       " (33, 1),\n",
       " (34, 3),\n",
       " (35, 1),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 1),\n",
       " (39, 1),\n",
       " (40, 1),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 1),\n",
       " (45, 1),\n",
       " (46, 1),\n",
       " (47, 1),\n",
       " (48, 1),\n",
       " (49, 2),\n",
       " (50, 4),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 2),\n",
       " (54, 1),\n",
       " (55, 2),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 2)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 52 (\"support\") appears 2 time.\n",
      "Word 82 (\"improv\") appears 1 time.\n",
      "Word 89 (\"method\") appears 1 time.\n",
      "Word 125 (\"power\") appears 1 time.\n",
      "Word 134 (\"applic\") appears 1 time.\n",
      "Word 143 (\"ieee\") appears 1 time.\n",
      "Word 153 (\"problem\") appears 1 time.\n",
      "Word 154 (\"propos\") appears 1 time.\n",
      "Word 169 (\"experi\") appears 1 time.\n",
      "Word 179 (\"make\") appears 1 time.\n",
      "Word 192 (\"achiev\") appears 1 time.\n",
      "Word 225 (\"generat\") appears 1 time.\n",
      "Word 227 (\"idea\") appears 1 time.\n",
      "Word 244 (\"signific\") appears 1 time.\n",
      "Word 313 (\"difficult\") appears 1 time.\n",
      "Word 368 (\"import\") appears 1 time.\n",
      "Word 370 (\"introduc\") appears 1 time.\n",
      "Word 382 (\"class\") appears 6 time.\n",
      "Word 407 (\"emerg\") appears 2 time.\n",
      "Word 414 (\"tree\") appears 2 time.\n",
      "Word 416 (\"correct\") appears 1 time.\n",
      "Word 460 (\"approach\") appears 1 time.\n",
      "Word 473 (\"decis\") appears 2 time.\n",
      "Word 557 (\"occur\") appears 1 time.\n",
      "Word 582 (\"pattern\") appears 2 time.\n",
      "Word 624 (\"case\") appears 3 time.\n",
      "Word 630 (\"qualiti\") appears 1 time.\n",
      "Word 644 (\"instanc\") appears 2 time.\n",
      "Word 651 (\"employ\") appears 1 time.\n",
      "Word 661 (\"outperform\") appears 1 time.\n",
      "Word 677 (\"sampl\") appears 1 time.\n",
      "Word 719 (\"tradit\") appears 1 time.\n",
      "Word 728 (\"real\") appears 1 time.\n",
      "Word 770 (\"exist\") appears 1 time.\n",
      "Word 869 (\"higher\") appears 1 time.\n",
      "Word 928 (\"classifi\") appears 3 time.\n",
      "Word 970 (\"face\") appears 1 time.\n",
      "Word 990 (\"rare\") appears 7 time.\n",
      "Word 1311 (\"classif\") appears 4 time.\n",
      "Word 1377 (\"life\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_200 = bow_corpus[200]\n",
    "\n",
    "for i in range(len(bow_doc_200)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_200[i][0], \n",
    "                                                     dictionary[bow_doc_200[i][0]], \n",
    "                                                     bow_doc_200[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.126376808266365),\n",
      " (1, 0.07787565055114672),\n",
      " (2, 0.0505504023030399),\n",
      " (3, 0.07096209915928124),\n",
      " (4, 0.1748251902363385),\n",
      " (5, 0.04688443567726373),\n",
      " (6, 0.4359943602628918),\n",
      " (7, 0.05340127524193944),\n",
      " (8, 0.061323191701547604),\n",
      " (9, 0.06634824688343606),\n",
      " (10, 0.15057628619104355),\n",
      " (11, 0.14691563847129344),\n",
      " (12, 0.11439273585975188),\n",
      " (13, 0.09021621346066433),\n",
      " (14, 0.16542145878952677),\n",
      " (15, 0.04763464920893705),\n",
      " (16, 0.09262477331378463),\n",
      " (17, 0.07998269523208633),\n",
      " (18, 0.032805413275128516),\n",
      " (19, 0.0522628878867586),\n",
      " (20, 0.09326819024973004),\n",
      " (21, 0.03370998865714734),\n",
      " (22, 0.11400664596594824),\n",
      " (23, 0.20564612977768015),\n",
      " (24, 0.23875104747687836),\n",
      " (25, 0.10655828035125481),\n",
      " (26, 0.10185786188827148),\n",
      " (27, 0.11566174420557522),\n",
      " (28, 0.08111274960634392),\n",
      " (29, 0.08318625106719792),\n",
      " (30, 0.1078647813944015),\n",
      " (31, 0.052939184698755956),\n",
      " (32, 0.2512147198997714),\n",
      " (33, 0.06634824688343606),\n",
      " (34, 0.1514966249528634),\n",
      " (35, 0.10233543592587072),\n",
      " (36, 0.0946112632906696),\n",
      " (37, 0.06198975627106705),\n",
      " (38, 0.09835512272415922),\n",
      " (39, 0.126376808266365),\n",
      " (40, 0.05357696397479613),\n",
      " (41, 0.055032616966593575),\n",
      " (42, 0.12505293250810454),\n",
      " (43, 0.03452186657110125),\n",
      " (44, 0.026577911857854075),\n",
      " (45, 0.11744427891649116),\n",
      " (46, 0.10598636190530134),\n",
      " (47, 0.09567228178601846),\n",
      " (48, 0.05935583650129941),\n",
      " (49, 0.0983957722111673),\n",
      " (50, 0.34906735541470413),\n",
      " (51, 0.06165407618928791),\n",
      " (52, 0.038698279600595555),\n",
      " (53, 0.21672797634176705),\n",
      " (54, 0.11566174420557522),\n",
      " (55, 0.10969115620353585),\n",
      " (56, 0.03275750938995209),\n",
      " (57, 0.07594440572919868),\n",
      " (58, 0.1444841336183521)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.013*\"data\" + 0.012*\"approach\" + 0.010*\"detect\" + 0.010*\"event\" + 0.009*\"base\" + 0.009*\"time\" + 0.008*\"program\" + 0.008*\"analysi\" + 0.008*\"propos\" + 0.008*\"applic\"\n",
      "Topic: 1 \n",
      "Words: 0.010*\"propos\" + 0.009*\"agent\" + 0.009*\"method\" + 0.008*\"document\" + 0.008*\"similar\" + 0.008*\"game\" + 0.008*\"term\" + 0.007*\"effect\" + 0.007*\"model\" + 0.007*\"algorithm\"\n",
      "Topic: 2 \n",
      "Words: 0.022*\"inform\" + 0.015*\"user\" + 0.011*\"data\" + 0.009*\"provid\" + 0.009*\"research\" + 0.008*\"evalu\" + 0.008*\"base\" + 0.007*\"network\" + 0.007*\"collect\" + 0.007*\"search\"\n",
      "Topic: 3 \n",
      "Words: 0.019*\"queri\" + 0.018*\"data\" + 0.016*\"process\" + 0.014*\"cloud\" + 0.009*\"propos\" + 0.008*\"time\" + 0.008*\"algorithm\" + 0.008*\"cost\" + 0.008*\"effici\" + 0.007*\"problem\"\n",
      "Topic: 4 \n",
      "Words: 0.011*\"data\" + 0.011*\"test\" + 0.008*\"base\" + 0.008*\"method\" + 0.007*\"analysi\" + 0.007*\"provid\" + 0.007*\"research\" + 0.006*\"cluster\" + 0.006*\"perform\" + 0.006*\"structur\"\n",
      "Topic: 5 \n",
      "Words: 0.009*\"servic\" + 0.009*\"evalu\" + 0.009*\"base\" + 0.009*\"time\" + 0.008*\"resourc\" + 0.008*\"data\" + 0.007*\"result\" + 0.007*\"research\" + 0.007*\"algorithm\" + 0.007*\"method\"\n",
      "Topic: 6 \n",
      "Words: 0.009*\"data\" + 0.009*\"design\" + 0.008*\"network\" + 0.008*\"technolog\" + 0.008*\"social\" + 0.008*\"studi\" + 0.007*\"method\" + 0.007*\"predict\" + 0.007*\"perform\" + 0.007*\"interact\"\n",
      "Topic: 7 \n",
      "Words: 0.028*\"model\" + 0.019*\"process\" + 0.016*\"data\" + 0.013*\"approach\" + 0.011*\"base\" + 0.011*\"cloud\" + 0.010*\"applic\" + 0.009*\"perform\" + 0.008*\"propos\" + 0.008*\"time\"\n",
      "Topic: 8 \n",
      "Words: 0.025*\"model\" + 0.012*\"data\" + 0.009*\"base\" + 0.008*\"comput\" + 0.007*\"word\" + 0.007*\"method\" + 0.007*\"technolog\" + 0.007*\"inform\" + 0.006*\"time\" + 0.006*\"agent\"\n",
      "Topic: 9 \n",
      "Words: 0.015*\"agent\" + 0.013*\"base\" + 0.013*\"design\" + 0.009*\"model\" + 0.009*\"inform\" + 0.008*\"interact\" + 0.007*\"system\" + 0.007*\"data\" + 0.007*\"present\" + 0.006*\"approach\"\n",
      "Topic: 10 \n",
      "Words: 0.027*\"cloud\" + 0.016*\"servic\" + 0.015*\"comput\" + 0.015*\"applic\" + 0.012*\"data\" + 0.012*\"resourc\" + 0.010*\"base\" + 0.010*\"provid\" + 0.009*\"perform\" + 0.007*\"user\"\n",
      "Topic: 11 \n",
      "Words: 0.013*\"technolog\" + 0.012*\"studi\" + 0.010*\"protein\" + 0.008*\"interact\" + 0.007*\"inform\" + 0.007*\"develop\" + 0.007*\"approach\" + 0.007*\"predict\" + 0.006*\"research\" + 0.006*\"understand\"\n",
      "Topic: 12 \n",
      "Words: 0.029*\"model\" + 0.015*\"process\" + 0.012*\"servic\" + 0.012*\"algorithm\" + 0.009*\"base\" + 0.008*\"effici\" + 0.008*\"result\" + 0.007*\"applic\" + 0.007*\"data\" + 0.007*\"improv\"\n",
      "Topic: 13 \n",
      "Words: 0.015*\"user\" + 0.013*\"network\" + 0.010*\"energi\" + 0.009*\"model\" + 0.009*\"protocol\" + 0.009*\"propos\" + 0.009*\"base\" + 0.009*\"applic\" + 0.008*\"approach\" + 0.008*\"sensor\"\n",
      "Topic: 14 \n",
      "Words: 0.016*\"locat\" + 0.015*\"user\" + 0.014*\"data\" + 0.014*\"mobil\" + 0.012*\"queri\" + 0.011*\"base\" + 0.011*\"privaci\" + 0.010*\"algorithm\" + 0.007*\"studi\" + 0.007*\"inform\"\n",
      "Topic: 15 \n",
      "Words: 0.010*\"research\" + 0.009*\"particip\" + 0.009*\"data\" + 0.008*\"review\" + 0.008*\"design\" + 0.007*\"develop\" + 0.007*\"inform\" + 0.007*\"studi\" + 0.007*\"digit\" + 0.006*\"behavior\"\n",
      "Topic: 16 \n",
      "Words: 0.012*\"queri\" + 0.010*\"approach\" + 0.009*\"method\" + 0.009*\"base\" + 0.009*\"studi\" + 0.008*\"propos\" + 0.008*\"problem\" + 0.007*\"data\" + 0.007*\"user\" + 0.007*\"region\"\n",
      "Topic: 17 \n",
      "Words: 0.021*\"data\" + 0.011*\"algorithm\" + 0.010*\"time\" + 0.009*\"result\" + 0.009*\"research\" + 0.008*\"comput\" + 0.008*\"method\" + 0.006*\"model\" + 0.006*\"problem\" + 0.006*\"base\"\n",
      "Topic: 18 \n",
      "Words: 0.023*\"data\" + 0.016*\"resourc\" + 0.012*\"network\" + 0.011*\"base\" + 0.011*\"propos\" + 0.010*\"cloud\" + 0.010*\"cluster\" + 0.010*\"user\" + 0.009*\"applic\" + 0.009*\"provid\"\n",
      "Topic: 19 \n",
      "Words: 0.018*\"process\" + 0.017*\"model\" + 0.014*\"method\" + 0.013*\"base\" + 0.011*\"data\" + 0.010*\"techniqu\" + 0.008*\"detect\" + 0.008*\"propos\" + 0.007*\"approach\" + 0.007*\"sequenc\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=15, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.006*\"process\" + 0.005*\"model\" + 0.004*\"data\" + 0.004*\"servic\" + 0.004*\"graph\" + 0.004*\"cloud\" + 0.004*\"network\" + 0.004*\"queri\" + 0.003*\"secur\" + 0.003*\"problem\" + 0.003*\"research\" + 0.003*\"resourc\" + 0.003*\"base\" + 0.003*\"inform\" + 0.003*\"approach\" + 0.003*\"algorithm\" + 0.003*\"applic\" + 0.003*\"structur\" + 0.003*\"thing\" + 0.003*\"detect\"\n",
      "Topic: 1 Word: 0.007*\"cloud\" + 0.006*\"detect\" + 0.005*\"energi\" + 0.005*\"data\" + 0.004*\"comput\" + 0.004*\"servic\" + 0.004*\"anomali\" + 0.004*\"resourc\" + 0.003*\"center\" + 0.003*\"network\" + 0.003*\"algorithm\" + 0.003*\"applic\" + 0.003*\"user\" + 0.003*\"method\" + 0.003*\"approach\" + 0.003*\"technolog\" + 0.003*\"research\" + 0.003*\"time\" + 0.003*\"queri\" + 0.003*\"attack\"\n",
      "Topic: 2 Word: 0.006*\"agent\" + 0.004*\"social\" + 0.004*\"model\" + 0.004*\"process\" + 0.003*\"user\" + 0.003*\"test\" + 0.003*\"method\" + 0.003*\"data\" + 0.003*\"servic\" + 0.003*\"algorithm\" + 0.003*\"interact\" + 0.003*\"predict\" + 0.003*\"event\" + 0.003*\"visual\" + 0.003*\"studi\" + 0.003*\"detect\" + 0.003*\"game\" + 0.003*\"approach\" + 0.002*\"question\" + 0.002*\"research\"\n",
      "Topic: 3 Word: 0.004*\"interact\" + 0.004*\"game\" + 0.004*\"user\" + 0.004*\"sequenc\" + 0.004*\"technolog\" + 0.004*\"train\" + 0.004*\"famili\" + 0.004*\"data\" + 0.003*\"network\" + 0.003*\"social\" + 0.003*\"surgic\" + 0.003*\"design\" + 0.003*\"model\" + 0.003*\"document\" + 0.003*\"semant\" + 0.003*\"method\" + 0.003*\"perform\" + 0.003*\"simul\" + 0.003*\"base\" + 0.003*\"optim\"\n",
      "Topic: 4 Word: 0.008*\"cloud\" + 0.004*\"queri\" + 0.004*\"data\" + 0.004*\"model\" + 0.004*\"process\" + 0.004*\"algorithm\" + 0.004*\"resourc\" + 0.004*\"servic\" + 0.004*\"user\" + 0.003*\"comput\" + 0.003*\"secur\" + 0.003*\"method\" + 0.003*\"topic\" + 0.003*\"applic\" + 0.003*\"cost\" + 0.003*\"locat\" + 0.003*\"approach\" + 0.003*\"privaci\" + 0.003*\"propos\" + 0.003*\"problem\"\n",
      "Topic: 5 Word: 0.005*\"model\" + 0.004*\"gene\" + 0.004*\"technolog\" + 0.003*\"process\" + 0.003*\"data\" + 0.003*\"adult\" + 0.003*\"method\" + 0.003*\"cloud\" + 0.003*\"older\" + 0.003*\"user\" + 0.003*\"learn\" + 0.003*\"research\" + 0.003*\"tool\" + 0.003*\"topic\" + 0.003*\"digit\" + 0.003*\"design\" + 0.003*\"network\" + 0.003*\"attack\" + 0.003*\"interact\" + 0.003*\"studi\"\n",
      "Topic: 6 Word: 0.004*\"data\" + 0.004*\"user\" + 0.004*\"search\" + 0.004*\"graph\" + 0.004*\"imag\" + 0.004*\"research\" + 0.003*\"inform\" + 0.003*\"retriev\" + 0.003*\"clinic\" + 0.003*\"queri\" + 0.003*\"brows\" + 0.003*\"method\" + 0.003*\"grid\" + 0.003*\"servic\" + 0.003*\"cloud\" + 0.003*\"health\" + 0.003*\"applic\" + 0.003*\"project\" + 0.003*\"secur\" + 0.003*\"support\"\n",
      "Topic: 7 Word: 0.005*\"data\" + 0.004*\"queri\" + 0.004*\"privaci\" + 0.004*\"detect\" + 0.004*\"sensor\" + 0.003*\"network\" + 0.003*\"user\" + 0.003*\"measur\" + 0.003*\"game\" + 0.003*\"anim\" + 0.003*\"cluster\" + 0.003*\"applic\" + 0.003*\"visual\" + 0.003*\"model\" + 0.003*\"time\" + 0.003*\"inform\" + 0.003*\"algorithm\" + 0.003*\"gaze\" + 0.002*\"design\" + 0.002*\"activ\"\n",
      "Topic: 8 Word: 0.005*\"cluster\" + 0.005*\"data\" + 0.004*\"process\" + 0.004*\"servic\" + 0.004*\"network\" + 0.004*\"model\" + 0.003*\"inform\" + 0.003*\"featur\" + 0.003*\"learn\" + 0.003*\"technolog\" + 0.003*\"algorithm\" + 0.003*\"agent\" + 0.003*\"user\" + 0.003*\"method\" + 0.003*\"read\" + 0.003*\"base\" + 0.003*\"rule\" + 0.003*\"cloud\" + 0.003*\"analysi\" + 0.003*\"approach\"\n",
      "Topic: 9 Word: 0.006*\"queri\" + 0.005*\"user\" + 0.004*\"search\" + 0.004*\"mobil\" + 0.004*\"model\" + 0.004*\"librari\" + 0.004*\"sequenc\" + 0.003*\"data\" + 0.003*\"document\" + 0.003*\"resourc\" + 0.003*\"base\" + 0.003*\"cloud\" + 0.003*\"evalu\" + 0.003*\"approach\" + 0.003*\"method\" + 0.003*\"inform\" + 0.003*\"servic\" + 0.003*\"effect\" + 0.003*\"propos\" + 0.003*\"index\"\n",
      "Topic: 10 Word: 0.005*\"word\" + 0.004*\"sens\" + 0.004*\"task\" + 0.004*\"secur\" + 0.003*\"mutat\" + 0.003*\"model\" + 0.003*\"data\" + 0.003*\"document\" + 0.003*\"inform\" + 0.003*\"technolog\" + 0.003*\"algorithm\" + 0.003*\"user\" + 0.003*\"interact\" + 0.003*\"system\" + 0.003*\"energi\" + 0.003*\"test\" + 0.003*\"network\" + 0.003*\"queri\" + 0.002*\"research\" + 0.002*\"comput\"\n",
      "Topic: 11 Word: 0.008*\"cloud\" + 0.005*\"resourc\" + 0.004*\"data\" + 0.004*\"user\" + 0.004*\"comput\" + 0.004*\"model\" + 0.004*\"collect\" + 0.004*\"manag\" + 0.003*\"schedul\" + 0.003*\"applic\" + 0.003*\"servic\" + 0.003*\"workflow\" + 0.003*\"document\" + 0.003*\"simul\" + 0.003*\"grid\" + 0.003*\"languag\" + 0.003*\"environ\" + 0.003*\"price\" + 0.003*\"devic\" + 0.003*\"system\"\n",
      "Topic: 12 Word: 0.005*\"model\" + 0.004*\"network\" + 0.004*\"data\" + 0.004*\"agent\" + 0.004*\"process\" + 0.004*\"energi\" + 0.004*\"sensor\" + 0.004*\"mobil\" + 0.004*\"cloud\" + 0.003*\"servic\" + 0.003*\"health\" + 0.003*\"applic\" + 0.003*\"user\" + 0.003*\"social\" + 0.003*\"inform\" + 0.003*\"offload\" + 0.003*\"algorithm\" + 0.003*\"method\" + 0.003*\"design\" + 0.003*\"approach\"\n",
      "Topic: 13 Word: 0.005*\"model\" + 0.004*\"process\" + 0.004*\"data\" + 0.004*\"pattern\" + 0.004*\"fuzzi\" + 0.003*\"agent\" + 0.003*\"tree\" + 0.003*\"network\" + 0.003*\"method\" + 0.003*\"algorithm\" + 0.003*\"design\" + 0.003*\"time\" + 0.003*\"techniqu\" + 0.003*\"user\" + 0.003*\"perform\" + 0.003*\"cluster\" + 0.003*\"base\" + 0.003*\"search\" + 0.003*\"cloud\" + 0.003*\"approach\"\n",
      "Topic: 14 Word: 0.006*\"cod\" + 0.005*\"cloud\" + 0.004*\"display\" + 0.004*\"compress\" + 0.004*\"retin\" + 0.004*\"algorithm\" + 0.003*\"model\" + 0.003*\"vessel\" + 0.003*\"method\" + 0.003*\"program\" + 0.003*\"secur\" + 0.003*\"resourc\" + 0.003*\"comput\" + 0.003*\"data\" + 0.003*\"design\" + 0.003*\"applic\" + 0.003*\"process\" + 0.003*\"optim\" + 0.003*\"servic\" + 0.003*\"search\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1,20):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supporting grid-based clinical trials in Scotland:A computational infrastructure to underpin complex clinical trials and medical population studies is highly desirable. This should allow access to a range of distributed clinical data sets; support the efficient processing and analysis of the data obtained; have security at its heart; and ensure that authorized individuals are able to see privileged data and no more. Each clinical trial has its own requirements on data sets and how they are used; hence a reusable and flexible framework offers many advantages. The MRC funded Virtual Organisations for Trials and Epidemiological Studies (VOTES) is a collaborative project involving several UK universities specifically to explore this space. This article presents the experiences of developing the Scottish component of this nationwide infrastructure, by the National e-Science Centre (NeSC) based at the University of Glasgow, and the issues inherent in accessing and using the clinical data sets in a flexible, dynamic and secure manner. © 2008 Sage Publications.\n",
      "\n",
      "Score: 0.7382935285568237\t \n",
      "Topic: 0.004*\"data\" + 0.004*\"user\" + 0.004*\"search\" + 0.004*\"graph\" + 0.004*\"imag\" + 0.004*\"research\" + 0.003*\"inform\" + 0.003*\"retriev\" + 0.003*\"clinic\" + 0.003*\"queri\" + 0.003*\"brows\" + 0.003*\"method\" + 0.003*\"grid\" + 0.003*\"servic\" + 0.003*\"cloud\" + 0.003*\"health\" + 0.003*\"applic\" + 0.003*\"project\" + 0.003*\"secur\" + 0.003*\"support\" + 0.003*\"time\" + 0.003*\"algorithm\" + 0.003*\"social\" + 0.003*\"cluster\" + 0.003*\"ethic\" + 0.003*\"approach\" + 0.002*\"anomali\" + 0.002*\"model\" + 0.002*\"design\" + 0.002*\"patient\"\n",
      "\n",
      "Score: 0.20351684093475342\t \n",
      "Topic: 0.008*\"cloud\" + 0.004*\"queri\" + 0.004*\"data\" + 0.004*\"model\" + 0.004*\"process\" + 0.004*\"algorithm\" + 0.004*\"resourc\" + 0.004*\"servic\" + 0.004*\"user\" + 0.003*\"comput\" + 0.003*\"secur\" + 0.003*\"method\" + 0.003*\"topic\" + 0.003*\"applic\" + 0.003*\"cost\" + 0.003*\"locat\" + 0.003*\"approach\" + 0.003*\"privaci\" + 0.003*\"propos\" + 0.003*\"problem\" + 0.003*\"base\" + 0.003*\"techniqu\" + 0.003*\"stream\" + 0.003*\"system\" + 0.003*\"effici\" + 0.003*\"provid\" + 0.003*\"workflow\" + 0.003*\"optim\" + 0.003*\"schedul\" + 0.003*\"time\"\n",
      "\n",
      "Score: 0.04831117391586304\t \n",
      "Topic: 0.006*\"process\" + 0.005*\"model\" + 0.004*\"data\" + 0.004*\"servic\" + 0.004*\"graph\" + 0.004*\"cloud\" + 0.004*\"network\" + 0.004*\"queri\" + 0.003*\"secur\" + 0.003*\"problem\" + 0.003*\"research\" + 0.003*\"resourc\" + 0.003*\"base\" + 0.003*\"inform\" + 0.003*\"approach\" + 0.003*\"algorithm\" + 0.003*\"applic\" + 0.003*\"structur\" + 0.003*\"thing\" + 0.003*\"detect\" + 0.003*\"time\" + 0.003*\"cluster\" + 0.003*\"analysi\" + 0.003*\"framework\" + 0.002*\"object\" + 0.002*\"internet\" + 0.002*\"region\" + 0.002*\"busi\" + 0.002*\"infrastructur\" + 0.002*\"access\"\n"
     ]
    }
   ],
   "source": [
    "i = 500\n",
    "\n",
    "def docText(documents,index):\n",
    "    text = documents[documents['index'] == index].values[0][0]\n",
    "    return text\n",
    "print(docText(documents,i))\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index,30)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 1), (42, 2), (48, 2), (67, 1), (84, 1), (96, 1), (103, 1), (143, 1), (160, 1), (193, 1), (199, 1), (204, 2), (262, 3), (312, 1), (317, 1), (348, 2), (349, 1), (414, 1), (435, 4), (436, 1), (456, 1), (483, 1), (545, 1), (678, 1), (864, 2), (918, 1), (939, 1), (940, 1), (968, 1), (970, 2), (1081, 1), (1210, 1), (1211, 1), (1212, 1), (1213, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(bow_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.4900451898574829\t \n",
      "Topic: 0.020*\"queri\" + 0.019*\"model\" + 0.013*\"method\" + 0.010*\"propos\" + 0.010*\"base\" + 0.009*\"network\" + 0.009*\"result\" + 0.008*\"data\" + 0.007*\"user\" + 0.007*\"time\"\n",
      "\n",
      "Score: 0.3876887261867523\t \n",
      "Topic: 0.016*\"applic\" + 0.015*\"resourc\" + 0.011*\"comput\" + 0.011*\"model\" + 0.010*\"cloud\" + 0.009*\"perform\" + 0.009*\"base\" + 0.008*\"process\" + 0.008*\"propos\" + 0.008*\"data\"\n",
      "\n",
      "Score: 0.10735906660556793\t \n",
      "Topic: 0.019*\"cloud\" + 0.017*\"servic\" + 0.010*\"resourc\" + 0.009*\"model\" + 0.009*\"user\" + 0.009*\"research\" + 0.009*\"provid\" + 0.008*\"comput\" + 0.008*\"data\" + 0.008*\"inform\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[100]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.42658671736717224\t \n",
      "Topic: 0.006*\"cloud\" + 0.005*\"process\" + 0.005*\"data\" + 0.004*\"model\" + 0.004*\"document\"\n",
      "\n",
      "Score: 0.3463570177555084\t \n",
      "Topic: 0.004*\"data\" + 0.003*\"model\" + 0.003*\"network\" + 0.003*\"query\" + 0.003*\"process\"\n",
      "\n",
      "Score: 0.1227799579501152\t \n",
      "Topic: 0.006*\"cod\" + 0.004*\"cloud\" + 0.004*\"function\" + 0.004*\"offload\" + 0.004*\"outlier\"\n",
      "\n",
      "Score: 0.05606284365057945\t \n",
      "Topic: 0.010*\"cloud\" + 0.005*\"social\" + 0.004*\"data\" + 0.004*\"service\" + 0.004*\"resource\"\n",
      "\n",
      "Score: 0.039035771042108536\t \n",
      "Topic: 0.005*\"cluster\" + 0.004*\"cloud\" + 0.004*\"algorithm\" + 0.004*\"service\" + 0.004*\"performance\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[3981]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3733212351799011\t Topic: 0.006*\"mobile\" + 0.005*\"data\" + 0.004*\"social\" + 0.004*\"privacy\" + 0.003*\"research\"\n",
      "Score: 0.26307183504104614\t Topic: 0.006*\"cloud\" + 0.005*\"process\" + 0.005*\"data\" + 0.004*\"model\" + 0.004*\"document\"\n",
      "Score: 0.16387544572353363\t Topic: 0.006*\"game\" + 0.005*\"data\" + 0.004*\"design\" + 0.003*\"network\" + 0.003*\"mutations\"\n",
      "Score: 0.11993960291147232\t Topic: 0.005*\"cluster\" + 0.004*\"model\" + 0.004*\"data\" + 0.004*\"program\" + 0.003*\"network\"\n",
      "Score: 0.06462649255990982\t Topic: 0.004*\"data\" + 0.003*\"model\" + 0.003*\"network\" + 0.003*\"query\" + 0.003*\"process\"\n"
     ]
    }
   ],
   "source": [
    "# unseen_document = \"machine learning\"\n",
    "unseen_document = \"The use of randomness in the designing of the digital devices has been discussed. Qualities of randomness such as unpredictability, indeterminacy and unexpectedness have been used as a creative resource to generate innovative , output. Randomness is a creative tool to inspire and generate innovative outputs that is a means to an end. The growth of digital interactivity has been accompanied by a increasing amount of interactive that express certain qualities of randomness during use. An emergent approach toward randomness is to allow users to interact directly with the randomness. Shuffle listening, which is an alternative listening mode offered by digital music players, is a more sophisticated approach, whereby application of randomness has publicly captured by imagination of many people. Considerations, in determining where a random feature can be used, should include the types of content, the domain and contexts where these digital devices are used\"\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
