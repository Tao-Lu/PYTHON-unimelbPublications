{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('title_abstract.csv', error_bad_lines=False);\n",
    "data_text = data[['text']]\n",
    "data_text['index'] = data_text.index\n",
    "data_text['paper_id'] = data['paperId']\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liaojinliang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating a list of stop words and adding custom stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "##Creating a list of custom stopwords\n",
    "new_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\"]\n",
    "stop_words = stop_words.union(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = []\n",
    "for i in range(0, len(documents)):\n",
    "    #Remove punctuations\n",
    "    preText = documents[documents['index'] == i].values[0][0]\n",
    "    text = re.sub('[^a-zA-Z]', ' ', preText)\n",
    "    \n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "    \n",
    "    ##Stemming\n",
    "    ps=PorterStemmer()\n",
    "    #Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words] \n",
    "    text = \" \".join(text)\n",
    "    corpus.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'understanding xcp equilibrium fairness prove xcp equilibrium solves constrained max min fairness problem identifying unique solution hierarchy optimization problem namely solved max min fair allocation solved xcp additional constraint describe algorithm compute equilibrium derive lower upper bound link utilization xcp reduces max min allocation single link network additional constraint cause flow receive arbitrarily small fraction max min allocation present simulation result confirm analytical finding ieee'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=10000, ngram_range=(1,3))\n",
    "X=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['strategy',\n",
       " 'managing',\n",
       " 'content',\n",
       " 'complexity',\n",
       " 'algorithm',\n",
       " 'animation',\n",
       " 'computer',\n",
       " 'excellent',\n",
       " 'medium',\n",
       " 'capturing']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cv.vocabulary_.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = get_top_n_words(corpus, n=50)\n",
    "top_df = pd.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Word  Freq\n",
      "0          data  3966\n",
      "1         based  2973\n",
      "2        system  2802\n",
      "3         model  2788\n",
      "4         paper  2336\n",
      "5        method  2266\n",
      "6          user  2216\n",
      "7      approach  2164\n",
      "8     algorithm  1953\n",
      "9          time  1818\n",
      "10        cloud  1814\n",
      "11  application  1763\n",
      "12  information  1758\n",
      "13      process  1724\n",
      "14       result  1607\n",
      "15      network  1558\n",
      "16        study  1549\n",
      "17      service  1541\n",
      "18          use  1522\n",
      "19     resource  1471\n",
      "20         used  1337\n",
      "21  performance  1318\n",
      "22    technique  1290\n",
      "23      problem  1255\n",
      "24     analysis  1253\n",
      "25      present  1250\n",
      "26     research  1195\n",
      "27        query  1160\n",
      "28    different  1092\n",
      "29   technology  1077\n",
      "30     proposed  1018\n",
      "31          set  1017\n",
      "32    computing  1006\n",
      "33      however   979\n",
      "34         task   968\n",
      "35       design   948\n",
      "36      propose   946\n",
      "37         ieee   936\n",
      "38      support   872\n",
      "39         work   865\n",
      "40       number   861\n",
      "41         real   856\n",
      "42       social   848\n",
      "43         cost   793\n",
      "44     existing   780\n",
      "45  environment   758\n",
      "46  interaction   757\n",
      "47    challenge   729\n",
      "48         term   728\n",
      "49       search   728\n"
     ]
    }
   ],
   "source": [
    "print(top_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Bi-gram  Freq\n",
      "0              paper present   425\n",
      "1              process model   412\n",
      "2            cloud computing   408\n",
      "3            springer verlag   359\n",
      "4             right reserved   302\n",
      "5                  state art   285\n",
      "6                  real time   251\n",
      "7                   data set   236\n",
      "8              paper propose   231\n",
      "9                 case study   230\n",
      "10          business process   218\n",
      "11               data center   215\n",
      "12       experimental result   211\n",
      "13         berlin heidelberg   204\n",
      "14             copyright acm   202\n",
      "15                real world   194\n",
      "16                  big data   188\n",
      "17             verlag berlin   176\n",
      "18          machine learning   160\n",
      "19              visual field   156\n",
      "20        energy consumption   152\n",
      "21           virtual machine   151\n",
      "22    springer international   149\n",
      "23  international publishing   149\n",
      "24             mobile device   147\n",
      "25            sensor network   144\n",
      "26     information retrieval   130\n",
      "27         anomaly detection   129\n",
      "28             cloud service   128\n",
      "29           proposed method   126\n",
      "30           quality service   121\n",
      "31            social network   118\n",
      "32                 event log   114\n",
      "33          nearest neighbor   112\n",
      "34            cloud resource   112\n",
      "35             social medium   110\n",
      "36                 real life   107\n",
      "37                john wiley   104\n",
      "38                 wiley son   104\n",
      "39               time series   103\n",
      "40           decision making   102\n",
      "41            public display   101\n",
      "42                well known    99\n",
      "43            cloud provider    98\n",
      "44             propose novel    97\n",
      "45           springer nature    96\n",
      "46                wide range    95\n",
      "47               data mining    95\n",
      "48            copyright held    95\n",
      "49              elsevier ltd    93\n"
     ]
    }
   ],
   "source": [
    "def get_top_n2_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(2,2),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "top2_words = get_top_n2_words(corpus, n=50)\n",
    "top2_df = pd.DataFrame(top2_words)\n",
    "top2_df.columns=[\"Bi-gram\", \"Freq\"]\n",
    "print(top2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Tri-gram  Freq\n",
      "0                  springer verlag berlin   176\n",
      "1                verlag berlin heidelberg   176\n",
      "2       springer international publishing   149\n",
      "3                          john wiley son   104\n",
      "4                       held owner author    91\n",
      "5                           wiley son ltd    90\n",
      "6                    copyright held owner    88\n",
      "7               springer science business    83\n",
      "8                 science business medium    83\n",
      "9                 wireless sensor network    82\n",
      "10        association computing machinery    72\n",
      "11                elsevier right reserved    71\n",
      "12                 business process model    69\n",
      "13                   copyright john wiley    68\n",
      "14                    quality service qos    68\n",
      "15   international publishing switzerland    68\n",
      "16            international publishing ag    67\n",
      "17                 support vector machine    63\n",
      "18                     ltd right reserved    62\n",
      "19            cloud computing environment    62\n",
      "20  association computational linguistics    61\n",
      "21                      cloud data center    60\n",
      "22                     elsevier ltd right    59\n",
      "23                   part springer nature    46\n",
      "24                service level agreement    42\n",
      "25                       state art method    40\n",
      "26            springer nature switzerland    40\n",
      "27                  nature switzerland ag    40\n",
      "28                     multi agent system    39\n",
      "29                     internet thing iot    38\n",
      "30                     inc right reserved    37\n",
      "31                    business medium llc    37\n",
      "32                 nearest neighbor query    36\n",
      "33                   taylor francis group    34\n",
      "34            natural language processing    33\n",
      "35                 mobile cloud computing    33\n",
      "36                     big data analytics    33\n",
      "37           information retrieval system    32\n",
      "38                oxford university press    32\n",
      "39                licensee biomed central    32\n",
      "40                 location based service    31\n",
      "41                     biomed central ltd    31\n",
      "42                     org right reserved    30\n",
      "43            published oxford university    30\n",
      "44   international federation information    29\n",
      "45      federation information processing    29\n",
      "46   information communication technology    29\n",
      "47                author published oxford    29\n",
      "48                 springer verlag london    28\n",
      "49                         et al licensee    28\n"
     ]
    }
   ],
   "source": [
    "#Most frequently occuring Tri-grams\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3), \n",
    "           max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n",
    "top3_words = get_top_n3_words(corpus, n=50)\n",
    "top3_df = pd.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Freq\"]\n",
    "print(top3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPaperID(index):\n",
    "    id = documents[documents['index'] == index].values[0][2]\n",
    "    return id[:-1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for sorting tf_idf in descending order\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# fetch document for which keywords needs to be extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKeyWords(index):\n",
    "    kws = list()\n",
    "    doc=corpus[index]\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "    # now print the results\n",
    "    for k in keywords:\n",
    "        if(keywords[k] > 0.20):\n",
    "            kws.append(k)\n",
    "    \n",
    "    return kws\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35421333 ['tree', 'splay', 'binary', 'search tree']\n"
     ]
    }
   ],
   "source": [
    "print(getPaperID(128),getKeyWords(128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import couchdb\n",
    "import csv\n",
    "\n",
    "couch=couchdb.Server(\"http://admin:password@localhost:5984\")\n",
    "try:\n",
    "    database=couch[\"paperinfo_scopus\"]\n",
    "except:\n",
    "    print(\"wrong db name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def updateDoc(paperid, keywords):\n",
    "    doc = database.get(str(paperid))\n",
    "    doc[\"keyword\"] = keywords\n",
    "    doc = database.save(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000036988\n",
      "0000036988 Saved\n",
      "0000764262\n",
      "0000764262 Saved\n",
      "0000891764\n",
      "0000891764 Saved\n",
      "0001104487\n",
      "0001104487 Saved\n",
      "0001624306\n",
      "0001624306 Saved\n",
      "0001790521\n",
      "0001790521 Saved\n",
      "0001825807\n",
      "0001825807 Saved\n",
      "0001919357\n",
      "0001919357 Saved\n",
      "0002124265\n",
      "0002124265 Saved\n",
      "0002848777\n",
      "0002848777 Saved\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    updateDoc(getPaperID(i),getKeyWords(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document '0001919357'@'5-a9539b7eb11c2ed62786b14ebe691ae7' {'title': 'Guidelines for Presentation and Comparison of Indexing Techniques', 'abstract': 'Descriptions of new indexing techniques are a common outcome of database research, but these descriptions are sometimes marred by poor methodology and a lack of comparison to other schemes. In this paper we describe a framework for presentation and comparison of indexing schemes that we believe sets a minimum standard for development and dissemination of research results in this area.', 'coverDate': '1996-01-01', 'coverDateYear': '1996', 'cite_count': '33', 'paper_type': 'Review', 'CISAuthors': '35586971600,56891817800,7003595103', 'co_author': ['7003595103', '35586971600', '56891817800'], 'type': 'Paper', 'keyword': ['indexing', 'comparison', 'presentation', 'description', 'scheme']}>\n"
     ]
    }
   ],
   "source": [
    "print(database.get(str(\"0001919357\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
