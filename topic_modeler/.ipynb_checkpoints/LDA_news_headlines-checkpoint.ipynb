{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('title_abstract.csv', error_bad_lines=False);\n",
    "data_text = data[['text']]\n",
    "data_text['index'] = data_text.index\n",
    "data_text['paper_id'] = data['paperId']\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3982"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A strategy for managing content complexity in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Efficient passage ranking for document databas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The aditi deductive database system:Deductive ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Housekeeping for prefix coding:We consider the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Memory efficient ranking:Fast and effective ra...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  index\n",
       "0  A strategy for managing content complexity in ...      0\n",
       "1  Efficient passage ranking for document databas...      1\n",
       "2  The aditi deductive database system:Deductive ...      2\n",
       "3  Housekeeping for prefix coding:We consider the...      3\n",
       "4  Memory efficient ranking:Fast and effective ra...      4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Using', 'emerging', 'patterns', 'and', 'decision', 'trees', 'in', 'rare-class', 'classification:The', 'problem', 'of', 'classifying', 'rarely', 'occurring', 'cases', 'is', 'faced', 'in', 'many', 'real', 'life', 'applications.', 'The', 'scarcity', 'of', 'the', 'rare', 'cases', 'makes', 'it', 'difficult', 'to', 'classify', 'them', 'correctly', 'using', 'traditional', 'classifiers.', 'In', 'this', 'paper,', 'we', 'propose', 'a', 'new', 'approach', 'to', 'use', 'emerging', 'patterns', '(EPs)', 'and', 'decision', 'trees', '(DTs)', 'in', 'rare-class', 'classification', '(EPDT).', 'EPs', 'are', 'those', 'itemsets', 'whose', 'supports', 'in', 'one', 'class', 'are', 'significantly', 'higher', 'than', 'their', 'supports', 'in', 'the', 'other', 'classes.', 'EPDT', 'employs', 'the', 'power', 'of', 'EPs', 'to', 'improve', 'the', 'quality', 'of', 'rare-case', 'classification.', 'To', 'achieve', 'this', 'aim,', 'we', 'first', 'introduce', 'the', 'idea', 'of', 'generating', 'new', 'non-existing', 'rare-class', 'instances,', 'and', 'then', 'we', 'over-sample', 'the', 'most', 'important', 'rare-class', 'instances.', 'Our', 'experiments', 'show', 'that', 'EPDT', 'outperforms', 'many', 'classification', 'methods.', '©', '2004', 'IEEE.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['emerge', 'pattern', 'decision', 'tree', 'rare', 'class', 'classification', 'problem', 'classify', 'rarely', 'occur', 'case', 'face', 'real', 'life', 'applications', 'scarcity', 'rare', 'case', 'make', 'difficult', 'classify', 'correctly', 'traditional', 'classifiers', 'paper', 'propose', 'approach', 'emerge', 'pattern', 'decision', 'tree', 'rare', 'class', 'classification', 'epdt', 'itemsets', 'support', 'class', 'significantly', 'higher', 'support', 'class', 'epdt', 'employ', 'power', 'improve', 'quality', 'rare', 'case', 'classification', 'achieve', 'introduce', 'idea', 'generate', 'exist', 'rare', 'class', 'instance', 'sample', 'important', 'rare', 'class', 'instance', 'experiment', 'epdt', 'outperform', 'classification', 'methods', 'ieee']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 200].values[0][0]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    [efficient, consumer, response, survey, austra...\n",
       "11    [binary, interpolative, cod, effective, index,...\n",
       "12    [empirical, evaluation, cod, methods, multi, s...\n",
       "13    [fast, algorithm, meld, splay, tree, springer,...\n",
       "14    [efficient, object, orient, program, prolog, d...\n",
       "15    [optimal, dynamic, multi, attribute, hash, ran...\n",
       "16    [determinism, functional, languages, introduct...\n",
       "17    [efficient, computation, query, stratify, data...\n",
       "18    [share, groundness, dependencies, logic, progr...\n",
       "19    [linear, arboricity, linear, arboricity, regul...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 accompani\n",
      "1 action\n",
      "2 address\n",
      "3 advantag\n",
      "4 algorithm\n",
      "5 allow\n",
      "6 anim\n",
      "7 avail\n",
      "8 call\n",
      "9 captur\n",
      "10 complex\n",
      "11 content\n",
      "12 control\n",
      "13 coordin\n",
      "14 correspond\n",
      "15 data\n",
      "16 descript\n",
      "17 detail\n",
      "18 differ\n",
      "19 dofferem\n",
      "20 dynam\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "# bow_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 52 (\"support\") appears 2 time.\n",
      "Word 82 (\"improv\") appears 1 time.\n",
      "Word 89 (\"method\") appears 1 time.\n",
      "Word 125 (\"power\") appears 1 time.\n",
      "Word 134 (\"applic\") appears 1 time.\n",
      "Word 143 (\"ieee\") appears 1 time.\n",
      "Word 153 (\"problem\") appears 1 time.\n",
      "Word 154 (\"propos\") appears 1 time.\n",
      "Word 169 (\"experi\") appears 1 time.\n",
      "Word 179 (\"make\") appears 1 time.\n",
      "Word 192 (\"achiev\") appears 1 time.\n",
      "Word 225 (\"generat\") appears 1 time.\n",
      "Word 227 (\"idea\") appears 1 time.\n",
      "Word 244 (\"signific\") appears 1 time.\n",
      "Word 313 (\"difficult\") appears 1 time.\n",
      "Word 368 (\"import\") appears 1 time.\n",
      "Word 370 (\"introduc\") appears 1 time.\n",
      "Word 382 (\"class\") appears 6 time.\n",
      "Word 407 (\"emerg\") appears 2 time.\n",
      "Word 414 (\"tree\") appears 2 time.\n",
      "Word 416 (\"correct\") appears 1 time.\n",
      "Word 460 (\"approach\") appears 1 time.\n",
      "Word 473 (\"decis\") appears 2 time.\n",
      "Word 557 (\"occur\") appears 1 time.\n",
      "Word 582 (\"pattern\") appears 2 time.\n",
      "Word 624 (\"case\") appears 3 time.\n",
      "Word 630 (\"qualiti\") appears 1 time.\n",
      "Word 644 (\"instanc\") appears 2 time.\n",
      "Word 651 (\"employ\") appears 1 time.\n",
      "Word 661 (\"outperform\") appears 1 time.\n",
      "Word 677 (\"sampl\") appears 1 time.\n",
      "Word 719 (\"tradit\") appears 1 time.\n",
      "Word 728 (\"real\") appears 1 time.\n",
      "Word 770 (\"exist\") appears 1 time.\n",
      "Word 869 (\"higher\") appears 1 time.\n",
      "Word 928 (\"classifi\") appears 3 time.\n",
      "Word 970 (\"face\") appears 1 time.\n",
      "Word 990 (\"rare\") appears 7 time.\n",
      "Word 1311 (\"classif\") appears 4 time.\n",
      "Word 1377 (\"life\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_200 = bow_corpus[200]\n",
    "\n",
    "for i in range(len(bow_doc_200)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_200[i][0], \n",
    "                                                     dictionary[bow_doc_200[i][0]], \n",
    "                                                     bow_doc_200[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.126376808266365),\n",
      " (1, 0.07787565055114672),\n",
      " (2, 0.0505504023030399),\n",
      " (3, 0.07096209915928124),\n",
      " (4, 0.1748251902363385),\n",
      " (5, 0.04688443567726373),\n",
      " (6, 0.4359943602628918),\n",
      " (7, 0.05340127524193944),\n",
      " (8, 0.061323191701547604),\n",
      " (9, 0.06634824688343606),\n",
      " (10, 0.15057628619104355),\n",
      " (11, 0.14691563847129344),\n",
      " (12, 0.11439273585975188),\n",
      " (13, 0.09021621346066433),\n",
      " (14, 0.16542145878952677),\n",
      " (15, 0.04763464920893705),\n",
      " (16, 0.09262477331378463),\n",
      " (17, 0.07998269523208633),\n",
      " (18, 0.032805413275128516),\n",
      " (19, 0.0522628878867586),\n",
      " (20, 0.09326819024973004),\n",
      " (21, 0.03370998865714734),\n",
      " (22, 0.11400664596594824),\n",
      " (23, 0.20564612977768015),\n",
      " (24, 0.23875104747687836),\n",
      " (25, 0.10655828035125481),\n",
      " (26, 0.10185786188827148),\n",
      " (27, 0.11566174420557522),\n",
      " (28, 0.08111274960634392),\n",
      " (29, 0.08318625106719792),\n",
      " (30, 0.1078647813944015),\n",
      " (31, 0.052939184698755956),\n",
      " (32, 0.2512147198997714),\n",
      " (33, 0.06634824688343606),\n",
      " (34, 0.1514966249528634),\n",
      " (35, 0.10233543592587072),\n",
      " (36, 0.0946112632906696),\n",
      " (37, 0.06198975627106705),\n",
      " (38, 0.09835512272415922),\n",
      " (39, 0.126376808266365),\n",
      " (40, 0.05357696397479613),\n",
      " (41, 0.055032616966593575),\n",
      " (42, 0.12505293250810454),\n",
      " (43, 0.03452186657110125),\n",
      " (44, 0.026577911857854075),\n",
      " (45, 0.11744427891649116),\n",
      " (46, 0.10598636190530134),\n",
      " (47, 0.09567228178601846),\n",
      " (48, 0.05935583650129941),\n",
      " (49, 0.0983957722111673),\n",
      " (50, 0.34906735541470413),\n",
      " (51, 0.06165407618928791),\n",
      " (52, 0.038698279600595555),\n",
      " (53, 0.21672797634176705),\n",
      " (54, 0.11566174420557522),\n",
      " (55, 0.10969115620353585),\n",
      " (56, 0.03275750938995209),\n",
      " (57, 0.07594440572919868),\n",
      " (58, 0.1444841336183521)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=20, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.013*\"data\" + 0.012*\"approach\" + 0.010*\"detect\" + 0.010*\"event\" + 0.009*\"base\" + 0.009*\"time\" + 0.008*\"program\" + 0.008*\"analysi\" + 0.008*\"propos\" + 0.008*\"applic\" + 0.007*\"comput\" + 0.007*\"method\" + 0.007*\"constraint\" + 0.007*\"techniqu\" + 0.006*\"result\" + 0.006*\"algorithm\" + 0.005*\"evalu\" + 0.005*\"problem\" + 0.005*\"queri\" + 0.005*\"process\" + 0.005*\"chang\" + 0.005*\"stream\" + 0.005*\"pattern\" + 0.005*\"execut\" + 0.004*\"task\" + 0.004*\"effici\" + 0.004*\"effect\" + 0.004*\"larg\" + 0.004*\"springer\" + 0.004*\"word\" + 0.004*\"differ\" + 0.004*\"model\" + 0.004*\"inform\" + 0.004*\"present\" + 0.004*\"logic\" + 0.004*\"mine\" + 0.004*\"graph\" + 0.004*\"real\" + 0.004*\"support\" + 0.004*\"system\" + 0.004*\"perform\" + 0.003*\"exist\" + 0.003*\"relat\" + 0.003*\"number\" + 0.003*\"depend\" + 0.003*\"domain\" + 0.003*\"import\" + 0.003*\"abstract\" + 0.003*\"high\" + 0.003*\"signific\"\n",
      "Topic: 1 \n",
      "Words: 0.010*\"propos\" + 0.009*\"agent\" + 0.009*\"method\" + 0.008*\"document\" + 0.008*\"similar\" + 0.008*\"game\" + 0.008*\"term\" + 0.007*\"effect\" + 0.007*\"model\" + 0.007*\"algorithm\" + 0.007*\"network\" + 0.007*\"data\" + 0.006*\"simul\" + 0.006*\"base\" + 0.006*\"result\" + 0.006*\"approach\" + 0.005*\"social\" + 0.005*\"time\" + 0.005*\"strategi\" + 0.005*\"requir\" + 0.005*\"inform\" + 0.005*\"design\" + 0.005*\"valu\" + 0.005*\"cooper\" + 0.005*\"system\" + 0.005*\"retriev\" + 0.005*\"collect\" + 0.005*\"specif\" + 0.004*\"applic\" + 0.004*\"provid\" + 0.004*\"comput\" + 0.004*\"techniqu\" + 0.004*\"number\" + 0.004*\"player\" + 0.004*\"properti\" + 0.004*\"queri\" + 0.004*\"analysi\" + 0.004*\"differ\" + 0.004*\"improv\" + 0.003*\"test\" + 0.003*\"index\" + 0.003*\"resourc\" + 0.003*\"high\" + 0.003*\"identifi\" + 0.003*\"load\" + 0.003*\"challeng\" + 0.003*\"sampl\" + 0.003*\"experi\" + 0.003*\"work\" + 0.003*\"play\"\n",
      "Topic: 2 \n",
      "Words: 0.022*\"inform\" + 0.015*\"user\" + 0.011*\"data\" + 0.009*\"provid\" + 0.009*\"research\" + 0.008*\"evalu\" + 0.008*\"base\" + 0.007*\"network\" + 0.007*\"collect\" + 0.007*\"search\" + 0.007*\"health\" + 0.007*\"document\" + 0.007*\"result\" + 0.006*\"traffic\" + 0.006*\"design\" + 0.006*\"servic\" + 0.006*\"support\" + 0.006*\"time\" + 0.005*\"differ\" + 0.005*\"measur\" + 0.005*\"retriev\" + 0.005*\"method\" + 0.005*\"studi\" + 0.005*\"relev\" + 0.005*\"effect\" + 0.005*\"interact\" + 0.004*\"real\" + 0.004*\"price\" + 0.004*\"model\" + 0.004*\"propos\" + 0.004*\"urban\" + 0.004*\"content\" + 0.004*\"onlin\" + 0.004*\"system\" + 0.004*\"experi\" + 0.004*\"present\" + 0.004*\"work\" + 0.004*\"rank\" + 0.004*\"identifi\" + 0.004*\"share\" + 0.004*\"need\" + 0.004*\"high\" + 0.004*\"type\" + 0.004*\"compar\" + 0.003*\"explor\" + 0.003*\"cloud\" + 0.003*\"approach\" + 0.003*\"technolog\" + 0.003*\"improv\" + 0.003*\"infrastructur\"\n",
      "Topic: 3 \n",
      "Words: 0.019*\"queri\" + 0.018*\"data\" + 0.016*\"process\" + 0.014*\"cloud\" + 0.009*\"propos\" + 0.008*\"time\" + 0.008*\"algorithm\" + 0.008*\"cost\" + 0.008*\"effici\" + 0.007*\"problem\" + 0.007*\"optim\" + 0.007*\"document\" + 0.007*\"servic\" + 0.007*\"result\" + 0.007*\"inform\" + 0.006*\"search\" + 0.006*\"model\" + 0.006*\"base\" + 0.006*\"comput\" + 0.006*\"distribut\" + 0.006*\"network\" + 0.006*\"retriev\" + 0.006*\"term\" + 0.006*\"provid\" + 0.005*\"scheme\" + 0.005*\"user\" + 0.005*\"approach\" + 0.005*\"secur\" + 0.005*\"method\" + 0.005*\"object\" + 0.005*\"system\" + 0.005*\"effect\" + 0.005*\"storag\" + 0.004*\"ieee\" + 0.004*\"solut\" + 0.004*\"access\" + 0.004*\"present\" + 0.004*\"structur\" + 0.004*\"techniqu\" + 0.004*\"applic\" + 0.004*\"springer\" + 0.004*\"reduc\" + 0.004*\"index\" + 0.004*\"text\" + 0.004*\"differ\" + 0.004*\"larg\" + 0.004*\"evalu\" + 0.004*\"perform\" + 0.004*\"requir\" + 0.003*\"file\"\n",
      "Topic: 4 \n",
      "Words: 0.011*\"data\" + 0.011*\"test\" + 0.008*\"base\" + 0.008*\"method\" + 0.007*\"analysi\" + 0.007*\"provid\" + 0.007*\"research\" + 0.006*\"cluster\" + 0.006*\"perform\" + 0.006*\"structur\" + 0.006*\"visual\" + 0.006*\"field\" + 0.005*\"inform\" + 0.005*\"measur\" + 0.005*\"approach\" + 0.005*\"network\" + 0.005*\"result\" + 0.005*\"pattern\" + 0.004*\"design\" + 0.004*\"health\" + 0.004*\"specif\" + 0.004*\"studi\" + 0.004*\"techniqu\" + 0.004*\"effect\" + 0.004*\"comput\" + 0.004*\"propos\" + 0.004*\"framework\" + 0.004*\"system\" + 0.004*\"user\" + 0.004*\"model\" + 0.004*\"patient\" + 0.004*\"learn\" + 0.004*\"time\" + 0.004*\"evalu\" + 0.004*\"differ\" + 0.004*\"function\" + 0.004*\"present\" + 0.004*\"locat\" + 0.004*\"generat\" + 0.004*\"support\" + 0.003*\"associ\" + 0.003*\"real\" + 0.003*\"relat\" + 0.003*\"algorithm\" + 0.003*\"process\" + 0.003*\"improv\" + 0.003*\"social\" + 0.003*\"work\" + 0.003*\"clinic\" + 0.003*\"develop\"\n",
      "Topic: 5 \n",
      "Words: 0.009*\"servic\" + 0.009*\"evalu\" + 0.009*\"base\" + 0.009*\"time\" + 0.008*\"resourc\" + 0.008*\"data\" + 0.007*\"result\" + 0.007*\"research\" + 0.007*\"algorithm\" + 0.007*\"method\" + 0.007*\"provid\" + 0.007*\"user\" + 0.006*\"perform\" + 0.006*\"technolog\" + 0.006*\"propos\" + 0.006*\"develop\" + 0.006*\"databas\" + 0.006*\"dynam\" + 0.005*\"learn\" + 0.005*\"studi\" + 0.005*\"comput\" + 0.005*\"search\" + 0.005*\"graph\" + 0.005*\"cloud\" + 0.005*\"inform\" + 0.005*\"applic\" + 0.004*\"approach\" + 0.004*\"effici\" + 0.004*\"social\" + 0.004*\"model\" + 0.004*\"improv\" + 0.004*\"larg\" + 0.004*\"problem\" + 0.004*\"techniqu\" + 0.004*\"topic\" + 0.004*\"distribut\" + 0.004*\"human\" + 0.004*\"energi\" + 0.003*\"number\" + 0.003*\"tool\" + 0.003*\"relat\" + 0.003*\"real\" + 0.003*\"signific\" + 0.003*\"manag\" + 0.003*\"sequenc\" + 0.003*\"metric\" + 0.003*\"cost\" + 0.003*\"cluster\" + 0.003*\"increas\" + 0.003*\"sens\"\n",
      "Topic: 6 \n",
      "Words: 0.009*\"data\" + 0.009*\"design\" + 0.008*\"network\" + 0.008*\"technolog\" + 0.008*\"social\" + 0.008*\"studi\" + 0.007*\"method\" + 0.007*\"predict\" + 0.007*\"perform\" + 0.007*\"interact\" + 0.006*\"base\" + 0.006*\"provid\" + 0.006*\"approach\" + 0.005*\"result\" + 0.005*\"present\" + 0.005*\"support\" + 0.005*\"develop\" + 0.005*\"famili\" + 0.005*\"time\" + 0.005*\"effect\" + 0.004*\"function\" + 0.004*\"peopl\" + 0.004*\"game\" + 0.004*\"applic\" + 0.004*\"term\" + 0.004*\"allow\" + 0.004*\"system\" + 0.004*\"improv\" + 0.004*\"experi\" + 0.004*\"process\" + 0.004*\"evalu\" + 0.004*\"inform\" + 0.004*\"explor\" + 0.004*\"work\" + 0.004*\"differ\" + 0.004*\"model\" + 0.003*\"practic\" + 0.003*\"propos\" + 0.003*\"sensor\" + 0.003*\"includ\" + 0.003*\"structur\" + 0.003*\"identifi\" + 0.003*\"show\" + 0.003*\"larg\" + 0.003*\"user\" + 0.003*\"comput\" + 0.003*\"analysi\" + 0.003*\"associ\" + 0.003*\"algorithm\" + 0.003*\"physic\"\n",
      "Topic: 7 \n",
      "Words: 0.028*\"model\" + 0.019*\"process\" + 0.016*\"data\" + 0.013*\"approach\" + 0.011*\"base\" + 0.011*\"cloud\" + 0.010*\"applic\" + 0.009*\"perform\" + 0.008*\"propos\" + 0.008*\"time\" + 0.008*\"comput\" + 0.007*\"algorithm\" + 0.007*\"result\" + 0.006*\"method\" + 0.006*\"simul\" + 0.006*\"present\" + 0.005*\"resourc\" + 0.005*\"techniqu\" + 0.005*\"optim\" + 0.005*\"larg\" + 0.005*\"structur\" + 0.005*\"differ\" + 0.004*\"dynam\" + 0.004*\"compar\" + 0.004*\"graph\" + 0.004*\"configur\" + 0.004*\"requir\" + 0.004*\"event\" + 0.004*\"servic\" + 0.004*\"provid\" + 0.004*\"case\" + 0.004*\"scale\" + 0.004*\"implement\" + 0.004*\"sampl\" + 0.003*\"exist\" + 0.003*\"experi\" + 0.003*\"analysi\" + 0.003*\"busi\" + 0.003*\"workflow\" + 0.003*\"featur\" + 0.003*\"real\" + 0.003*\"measur\" + 0.003*\"effect\" + 0.003*\"behavior\" + 0.003*\"user\" + 0.003*\"allow\" + 0.003*\"springer\" + 0.003*\"develop\" + 0.003*\"schedul\" + 0.003*\"high\"\n",
      "Topic: 8 \n",
      "Words: 0.025*\"model\" + 0.012*\"data\" + 0.009*\"base\" + 0.008*\"comput\" + 0.007*\"word\" + 0.007*\"method\" + 0.007*\"technolog\" + 0.007*\"inform\" + 0.006*\"time\" + 0.006*\"agent\" + 0.006*\"propos\" + 0.006*\"interact\" + 0.006*\"provid\" + 0.005*\"file\" + 0.005*\"result\" + 0.005*\"human\" + 0.005*\"social\" + 0.004*\"task\" + 0.004*\"distribut\" + 0.004*\"approach\" + 0.004*\"specif\" + 0.004*\"present\" + 0.004*\"anim\" + 0.004*\"user\" + 0.004*\"languag\" + 0.004*\"develop\" + 0.004*\"sens\" + 0.004*\"size\" + 0.004*\"system\" + 0.004*\"scheme\" + 0.004*\"algorithm\" + 0.004*\"process\" + 0.004*\"relat\" + 0.004*\"featur\" + 0.004*\"design\" + 0.004*\"servic\" + 0.004*\"network\" + 0.004*\"cloud\" + 0.003*\"evalu\" + 0.003*\"studi\" + 0.003*\"analysi\" + 0.003*\"research\" + 0.003*\"compress\" + 0.003*\"perform\" + 0.003*\"associ\" + 0.003*\"high\" + 0.003*\"topic\" + 0.003*\"build\" + 0.003*\"document\" + 0.003*\"corpus\"\n",
      "Topic: 9 \n",
      "Words: 0.015*\"agent\" + 0.013*\"base\" + 0.013*\"design\" + 0.009*\"model\" + 0.009*\"inform\" + 0.008*\"interact\" + 0.007*\"system\" + 0.007*\"data\" + 0.007*\"present\" + 0.006*\"approach\" + 0.006*\"method\" + 0.006*\"secur\" + 0.006*\"student\" + 0.005*\"time\" + 0.005*\"effect\" + 0.005*\"task\" + 0.005*\"studi\" + 0.005*\"servic\" + 0.005*\"propos\" + 0.005*\"support\" + 0.005*\"environ\" + 0.005*\"provid\" + 0.005*\"differ\" + 0.005*\"develop\" + 0.004*\"perform\" + 0.004*\"user\" + 0.004*\"pattern\" + 0.004*\"comput\" + 0.004*\"display\" + 0.004*\"research\" + 0.004*\"technolog\" + 0.004*\"evalu\" + 0.004*\"public\" + 0.004*\"learn\" + 0.004*\"cloud\" + 0.004*\"mobil\" + 0.004*\"result\" + 0.004*\"experi\" + 0.004*\"challeng\" + 0.004*\"activ\" + 0.003*\"level\" + 0.003*\"process\" + 0.003*\"multi\" + 0.003*\"work\" + 0.003*\"architectur\" + 0.003*\"understand\" + 0.003*\"communic\" + 0.003*\"particip\" + 0.003*\"devic\" + 0.003*\"techniqu\"\n",
      "Topic: 10 \n",
      "Words: 0.027*\"cloud\" + 0.016*\"servic\" + 0.015*\"comput\" + 0.015*\"applic\" + 0.012*\"data\" + 0.012*\"resourc\" + 0.010*\"base\" + 0.010*\"provid\" + 0.009*\"perform\" + 0.007*\"user\" + 0.006*\"model\" + 0.006*\"differ\" + 0.006*\"manag\" + 0.006*\"studi\" + 0.005*\"network\" + 0.005*\"develop\" + 0.005*\"inform\" + 0.005*\"method\" + 0.005*\"propos\" + 0.005*\"result\" + 0.005*\"present\" + 0.005*\"research\" + 0.005*\"system\" + 0.004*\"environ\" + 0.004*\"relat\" + 0.004*\"evalu\" + 0.004*\"infrastructur\" + 0.004*\"challeng\" + 0.004*\"qualiti\" + 0.004*\"util\" + 0.004*\"semant\" + 0.004*\"approach\" + 0.004*\"measur\" + 0.004*\"privaci\" + 0.003*\"discuss\" + 0.003*\"support\" + 0.003*\"design\" + 0.003*\"exist\" + 0.003*\"effici\" + 0.003*\"workflow\" + 0.003*\"high\" + 0.003*\"time\" + 0.003*\"identifi\" + 0.003*\"analysi\" + 0.003*\"need\" + 0.003*\"technolog\" + 0.003*\"test\" + 0.003*\"techniqu\" + 0.003*\"experi\" + 0.003*\"rule\"\n",
      "Topic: 11 \n",
      "Words: 0.013*\"technolog\" + 0.012*\"studi\" + 0.010*\"protein\" + 0.008*\"interact\" + 0.007*\"inform\" + 0.007*\"develop\" + 0.007*\"approach\" + 0.007*\"predict\" + 0.006*\"research\" + 0.006*\"understand\" + 0.006*\"user\" + 0.006*\"activ\" + 0.006*\"result\" + 0.006*\"method\" + 0.005*\"adopt\" + 0.005*\"differ\" + 0.005*\"model\" + 0.005*\"task\" + 0.005*\"effect\" + 0.005*\"base\" + 0.005*\"provid\" + 0.005*\"mutat\" + 0.005*\"work\" + 0.005*\"present\" + 0.004*\"design\" + 0.004*\"manag\" + 0.004*\"identifi\" + 0.004*\"term\" + 0.004*\"problem\" + 0.004*\"challeng\" + 0.004*\"algorithm\" + 0.004*\"import\" + 0.004*\"comput\" + 0.004*\"experi\" + 0.004*\"relat\" + 0.004*\"function\" + 0.004*\"domain\" + 0.004*\"structur\" + 0.004*\"exist\" + 0.003*\"investig\" + 0.003*\"resourc\" + 0.003*\"case\" + 0.003*\"control\" + 0.003*\"larg\" + 0.003*\"time\" + 0.003*\"assess\" + 0.003*\"explor\" + 0.003*\"find\" + 0.003*\"social\" + 0.003*\"signific\"\n",
      "Topic: 12 \n",
      "Words: 0.029*\"model\" + 0.015*\"process\" + 0.012*\"servic\" + 0.012*\"algorithm\" + 0.009*\"base\" + 0.008*\"effici\" + 0.008*\"result\" + 0.007*\"applic\" + 0.007*\"data\" + 0.007*\"improv\" + 0.007*\"propos\" + 0.006*\"function\" + 0.006*\"resourc\" + 0.006*\"approach\" + 0.006*\"cost\" + 0.006*\"requir\" + 0.006*\"cloud\" + 0.006*\"plan\" + 0.005*\"differ\" + 0.005*\"present\" + 0.005*\"evalu\" + 0.005*\"techniqu\" + 0.005*\"larg\" + 0.005*\"provid\" + 0.005*\"user\" + 0.004*\"compress\" + 0.004*\"perform\" + 0.004*\"network\" + 0.004*\"busi\" + 0.004*\"analysi\" + 0.004*\"comput\" + 0.004*\"design\" + 0.004*\"problem\" + 0.004*\"time\" + 0.004*\"develop\" + 0.004*\"express\" + 0.003*\"task\" + 0.003*\"reduc\" + 0.003*\"detect\" + 0.003*\"studi\" + 0.003*\"effect\" + 0.003*\"composit\" + 0.003*\"import\" + 0.003*\"depend\" + 0.003*\"optim\" + 0.003*\"challeng\" + 0.003*\"method\" + 0.003*\"oper\" + 0.003*\"share\" + 0.003*\"manag\"\n",
      "Topic: 13 \n",
      "Words: 0.015*\"user\" + 0.013*\"network\" + 0.010*\"energi\" + 0.009*\"model\" + 0.009*\"protocol\" + 0.009*\"propos\" + 0.009*\"base\" + 0.009*\"applic\" + 0.008*\"approach\" + 0.008*\"sensor\" + 0.008*\"process\" + 0.008*\"data\" + 0.007*\"mobil\" + 0.007*\"resourc\" + 0.007*\"detect\" + 0.007*\"result\" + 0.006*\"system\" + 0.006*\"inform\" + 0.006*\"peer\" + 0.006*\"research\" + 0.006*\"search\" + 0.006*\"servic\" + 0.005*\"distribut\" + 0.005*\"manag\" + 0.005*\"algorithm\" + 0.005*\"ieee\" + 0.005*\"time\" + 0.004*\"effici\" + 0.004*\"differ\" + 0.004*\"perform\" + 0.004*\"challeng\" + 0.004*\"anomali\" + 0.004*\"problem\" + 0.004*\"develop\" + 0.004*\"consumpt\" + 0.004*\"provid\" + 0.004*\"exist\" + 0.004*\"real\" + 0.004*\"cluster\" + 0.004*\"present\" + 0.004*\"secur\" + 0.004*\"studi\" + 0.004*\"devic\" + 0.004*\"architectur\" + 0.004*\"cod\" + 0.004*\"communic\" + 0.003*\"evalu\" + 0.003*\"wireless\" + 0.003*\"comput\" + 0.003*\"improv\"\n",
      "Topic: 14 \n",
      "Words: 0.016*\"locat\" + 0.015*\"user\" + 0.014*\"data\" + 0.014*\"mobil\" + 0.012*\"queri\" + 0.011*\"base\" + 0.011*\"privaci\" + 0.010*\"algorithm\" + 0.007*\"studi\" + 0.007*\"inform\" + 0.007*\"approach\" + 0.007*\"method\" + 0.006*\"comput\" + 0.006*\"group\" + 0.006*\"trajectori\" + 0.006*\"provid\" + 0.005*\"cloud\" + 0.005*\"public\" + 0.005*\"time\" + 0.005*\"measur\" + 0.005*\"propos\" + 0.005*\"result\" + 0.005*\"collect\" + 0.005*\"applic\" + 0.005*\"network\" + 0.005*\"devic\" + 0.005*\"model\" + 0.005*\"research\" + 0.004*\"effici\" + 0.004*\"problem\" + 0.004*\"present\" + 0.004*\"communiti\" + 0.004*\"experi\" + 0.004*\"perform\" + 0.004*\"effect\" + 0.004*\"techniqu\" + 0.004*\"task\" + 0.004*\"attack\" + 0.004*\"predict\" + 0.004*\"object\" + 0.003*\"estim\" + 0.003*\"servic\" + 0.003*\"distanc\" + 0.003*\"individu\" + 0.003*\"index\" + 0.003*\"evalu\" + 0.003*\"space\" + 0.003*\"develop\" + 0.003*\"process\" + 0.003*\"health\"\n",
      "Topic: 15 \n",
      "Words: 0.010*\"research\" + 0.009*\"particip\" + 0.009*\"data\" + 0.008*\"review\" + 0.008*\"design\" + 0.007*\"develop\" + 0.007*\"inform\" + 0.007*\"studi\" + 0.007*\"digit\" + 0.006*\"behavior\" + 0.006*\"system\" + 0.006*\"present\" + 0.006*\"ontolog\" + 0.006*\"librari\" + 0.005*\"applic\" + 0.005*\"annot\" + 0.005*\"work\" + 0.005*\"user\" + 0.005*\"algorithm\" + 0.005*\"technolog\" + 0.005*\"support\" + 0.005*\"result\" + 0.004*\"provid\" + 0.004*\"relat\" + 0.004*\"text\" + 0.004*\"process\" + 0.004*\"model\" + 0.004*\"ethic\" + 0.004*\"crowdsourc\" + 0.004*\"tool\" + 0.004*\"evalu\" + 0.004*\"document\" + 0.004*\"differ\" + 0.004*\"knowledg\" + 0.004*\"author\" + 0.004*\"perform\" + 0.004*\"human\" + 0.003*\"set\" + 0.003*\"issu\" + 0.003*\"increas\" + 0.003*\"qualiti\" + 0.003*\"file\" + 0.003*\"task\" + 0.003*\"visual\" + 0.003*\"share\" + 0.003*\"current\" + 0.003*\"base\" + 0.003*\"approach\" + 0.003*\"assess\" + 0.003*\"challeng\"\n",
      "Topic: 16 \n",
      "Words: 0.012*\"queri\" + 0.010*\"approach\" + 0.009*\"method\" + 0.009*\"base\" + 0.009*\"studi\" + 0.008*\"propos\" + 0.008*\"problem\" + 0.007*\"data\" + 0.007*\"user\" + 0.007*\"region\" + 0.006*\"result\" + 0.006*\"comput\" + 0.006*\"effect\" + 0.006*\"algorithm\" + 0.005*\"sequenc\" + 0.005*\"function\" + 0.005*\"perform\" + 0.005*\"evalu\" + 0.005*\"patient\" + 0.005*\"applic\" + 0.005*\"larg\" + 0.005*\"optim\" + 0.004*\"time\" + 0.004*\"effici\" + 0.004*\"process\" + 0.004*\"provid\" + 0.004*\"present\" + 0.004*\"test\" + 0.004*\"research\" + 0.004*\"search\" + 0.004*\"improv\" + 0.004*\"type\" + 0.004*\"inform\" + 0.004*\"correl\" + 0.003*\"object\" + 0.003*\"system\" + 0.003*\"develop\" + 0.003*\"differ\" + 0.003*\"period\" + 0.003*\"literatur\" + 0.003*\"network\" + 0.003*\"number\" + 0.003*\"case\" + 0.003*\"learn\" + 0.003*\"diabet\" + 0.003*\"analysi\" + 0.003*\"resourc\" + 0.003*\"chang\" + 0.003*\"relat\" + 0.003*\"safe\"\n",
      "Topic: 17 \n",
      "Words: 0.021*\"data\" + 0.011*\"algorithm\" + 0.010*\"time\" + 0.009*\"result\" + 0.009*\"research\" + 0.008*\"comput\" + 0.008*\"method\" + 0.006*\"model\" + 0.006*\"problem\" + 0.006*\"base\" + 0.005*\"network\" + 0.005*\"present\" + 0.005*\"support\" + 0.005*\"social\" + 0.005*\"design\" + 0.005*\"technolog\" + 0.005*\"implement\" + 0.005*\"system\" + 0.005*\"structur\" + 0.005*\"grid\" + 0.005*\"program\" + 0.005*\"propos\" + 0.004*\"provid\" + 0.004*\"distribut\" + 0.004*\"requir\" + 0.004*\"develop\" + 0.004*\"applic\" + 0.004*\"resourc\" + 0.004*\"effici\" + 0.004*\"studi\" + 0.004*\"onlin\" + 0.004*\"access\" + 0.004*\"process\" + 0.004*\"differ\" + 0.004*\"communic\" + 0.003*\"user\" + 0.003*\"ieee\" + 0.003*\"function\" + 0.003*\"integr\" + 0.003*\"servic\" + 0.003*\"infrastructur\" + 0.003*\"interact\" + 0.003*\"secur\" + 0.003*\"inform\" + 0.003*\"high\" + 0.003*\"communiti\" + 0.003*\"case\" + 0.003*\"effect\" + 0.003*\"perform\" + 0.003*\"number\"\n",
      "Topic: 18 \n",
      "Words: 0.023*\"data\" + 0.016*\"resourc\" + 0.012*\"network\" + 0.011*\"base\" + 0.011*\"propos\" + 0.010*\"cloud\" + 0.010*\"cluster\" + 0.010*\"user\" + 0.009*\"applic\" + 0.009*\"provid\" + 0.008*\"schedul\" + 0.008*\"comput\" + 0.008*\"algorithm\" + 0.006*\"servic\" + 0.006*\"perform\" + 0.006*\"present\" + 0.006*\"time\" + 0.006*\"requir\" + 0.006*\"grid\" + 0.005*\"sensor\" + 0.005*\"result\" + 0.005*\"environ\" + 0.005*\"monitor\" + 0.005*\"ieee\" + 0.005*\"effect\" + 0.005*\"gene\" + 0.005*\"differ\" + 0.005*\"process\" + 0.004*\"distribut\" + 0.004*\"manag\" + 0.004*\"number\" + 0.004*\"evalu\" + 0.004*\"inform\" + 0.004*\"real\" + 0.004*\"virtual\" + 0.004*\"method\" + 0.004*\"approach\" + 0.004*\"improv\" + 0.004*\"devic\" + 0.004*\"cost\" + 0.003*\"model\" + 0.003*\"work\" + 0.003*\"object\" + 0.003*\"larg\" + 0.003*\"stream\" + 0.003*\"traffic\" + 0.003*\"util\" + 0.003*\"dynam\" + 0.003*\"signific\" + 0.003*\"system\"\n",
      "Topic: 19 \n",
      "Words: 0.018*\"process\" + 0.017*\"model\" + 0.014*\"method\" + 0.013*\"base\" + 0.011*\"data\" + 0.010*\"techniqu\" + 0.008*\"detect\" + 0.008*\"propos\" + 0.007*\"approach\" + 0.007*\"sequenc\" + 0.007*\"perform\" + 0.006*\"user\" + 0.006*\"cluster\" + 0.005*\"analysi\" + 0.005*\"result\" + 0.005*\"pattern\" + 0.005*\"provid\" + 0.005*\"featur\" + 0.005*\"differ\" + 0.005*\"present\" + 0.005*\"topic\" + 0.004*\"document\" + 0.004*\"similar\" + 0.004*\"evalu\" + 0.004*\"algorithm\" + 0.004*\"event\" + 0.004*\"measur\" + 0.004*\"inform\" + 0.004*\"classif\" + 0.004*\"effect\" + 0.004*\"studi\" + 0.004*\"imag\" + 0.004*\"task\" + 0.004*\"problem\" + 0.004*\"support\" + 0.004*\"relat\" + 0.004*\"mine\" + 0.004*\"springer\" + 0.004*\"improv\" + 0.004*\"classifi\" + 0.004*\"exist\" + 0.003*\"learn\" + 0.003*\"time\" + 0.003*\"experi\" + 0.003*\"predict\" + 0.003*\"discoveri\" + 0.003*\"dataset\" + 0.003*\"accuraci\" + 0.003*\"high\" + 0.003*\"identifi\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1,50):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Can you distinguish different topics using the words in each topic and their corresponding weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=15, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.005*\"cloud\" + 0.005*\"topic\" + 0.005*\"data\" + 0.004*\"network\" + 0.004*\"document\" + 0.004*\"user\" + 0.004*\"energi\" + 0.004*\"model\" + 0.003*\"search\" + 0.003*\"file\"\n",
      "\n",
      "Topic: 1 Word: 0.005*\"interact\" + 0.004*\"cluster\" + 0.004*\"model\" + 0.004*\"algorithm\" + 0.004*\"data\" + 0.004*\"network\" + 0.004*\"process\" + 0.004*\"peer\" + 0.003*\"social\" + 0.003*\"method\"\n",
      "\n",
      "Topic: 2 Word: 0.007*\"process\" + 0.005*\"queri\" + 0.005*\"model\" + 0.004*\"data\" + 0.004*\"search\" + 0.004*\"index\" + 0.004*\"array\" + 0.003*\"databas\" + 0.003*\"user\" + 0.003*\"mobil\"\n",
      "\n",
      "Topic: 3 Word: 0.008*\"cloud\" + 0.005*\"agent\" + 0.005*\"energi\" + 0.004*\"comput\" + 0.004*\"model\" + 0.004*\"resourc\" + 0.004*\"virtual\" + 0.004*\"public\" + 0.004*\"applic\" + 0.003*\"vote\"\n",
      "\n",
      "Topic: 4 Word: 0.010*\"queri\" + 0.004*\"trajectori\" + 0.004*\"locat\" + 0.004*\"network\" + 0.004*\"data\" + 0.004*\"graph\" + 0.004*\"model\" + 0.004*\"user\" + 0.004*\"measur\" + 0.003*\"algorithm\"\n",
      "\n",
      "Topic: 5 Word: 0.006*\"data\" + 0.005*\"detect\" + 0.005*\"stream\" + 0.004*\"anomali\" + 0.004*\"network\" + 0.004*\"mobil\" + 0.003*\"user\" + 0.003*\"game\" + 0.003*\"graph\" + 0.003*\"pattern\"\n",
      "\n",
      "Topic: 6 Word: 0.004*\"program\" + 0.004*\"secur\" + 0.004*\"agent\" + 0.004*\"model\" + 0.004*\"process\" + 0.003*\"user\" + 0.003*\"method\" + 0.003*\"inform\" + 0.003*\"technolog\" + 0.003*\"data\"\n",
      "\n",
      "Topic: 7 Word: 0.009*\"cloud\" + 0.007*\"servic\" + 0.007*\"resourc\" + 0.005*\"schedul\" + 0.005*\"workflow\" + 0.004*\"comput\" + 0.004*\"model\" + 0.004*\"cluster\" + 0.004*\"algorithm\" + 0.004*\"user\"\n",
      "\n",
      "Topic: 8 Word: 0.007*\"cloud\" + 0.005*\"workflow\" + 0.005*\"data\" + 0.005*\"model\" + 0.005*\"solver\" + 0.004*\"string\" + 0.004*\"portfolio\" + 0.004*\"process\" + 0.004*\"comput\" + 0.004*\"algorithm\"\n",
      "\n",
      "Topic: 9 Word: 0.005*\"technolog\" + 0.004*\"user\" + 0.004*\"model\" + 0.004*\"social\" + 0.004*\"data\" + 0.004*\"cloud\" + 0.004*\"design\" + 0.004*\"anim\" + 0.004*\"process\" + 0.003*\"research\"\n",
      "\n",
      "Topic: 10 Word: 0.005*\"data\" + 0.004*\"network\" + 0.004*\"attack\" + 0.004*\"trajectori\" + 0.004*\"detect\" + 0.004*\"research\" + 0.004*\"process\" + 0.003*\"ethic\" + 0.003*\"cloud\" + 0.003*\"health\"\n",
      "\n",
      "Topic: 11 Word: 0.004*\"model\" + 0.004*\"data\" + 0.004*\"inform\" + 0.004*\"queri\" + 0.003*\"method\" + 0.003*\"semant\" + 0.003*\"technolog\" + 0.003*\"resourc\" + 0.003*\"social\" + 0.003*\"text\"\n",
      "\n",
      "Topic: 12 Word: 0.005*\"document\" + 0.005*\"queri\" + 0.004*\"data\" + 0.004*\"word\" + 0.004*\"collect\" + 0.004*\"compress\" + 0.004*\"user\" + 0.004*\"text\" + 0.003*\"model\" + 0.003*\"test\"\n",
      "\n",
      "Topic: 13 Word: 0.005*\"agent\" + 0.005*\"cod\" + 0.004*\"inform\" + 0.003*\"user\" + 0.003*\"servic\" + 0.003*\"model\" + 0.003*\"cloud\" + 0.003*\"search\" + 0.003*\"product\" + 0.003*\"algorithm\"\n",
      "\n",
      "Topic: 14 Word: 0.005*\"model\" + 0.004*\"featur\" + 0.004*\"select\" + 0.004*\"learn\" + 0.004*\"train\" + 0.003*\"gene\" + 0.003*\"function\" + 0.003*\"process\" + 0.003*\"feedback\" + 0.003*\"data\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = list()\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.005*\"cloud\" + 0.005*\"topic\" + 0.005*\"data\" + 0.004*\"network\" + 0.004*\"document\" + 0.004*\"user\" + 0.004*\"energi\" + 0.004*\"model\" + 0.003*\"search\" + 0.003*\"file\"\n",
      "Topic: 1 Word: 0.005*\"interact\" + 0.004*\"cluster\" + 0.004*\"model\" + 0.004*\"algorithm\" + 0.004*\"data\" + 0.004*\"network\" + 0.004*\"process\" + 0.004*\"peer\" + 0.003*\"social\" + 0.003*\"method\"\n",
      "Topic: 2 Word: 0.007*\"process\" + 0.005*\"queri\" + 0.005*\"model\" + 0.004*\"data\" + 0.004*\"search\" + 0.004*\"index\" + 0.004*\"array\" + 0.003*\"databas\" + 0.003*\"user\" + 0.003*\"mobil\"\n",
      "Topic: 3 Word: 0.008*\"cloud\" + 0.005*\"agent\" + 0.005*\"energi\" + 0.004*\"comput\" + 0.004*\"model\" + 0.004*\"resourc\" + 0.004*\"virtual\" + 0.004*\"public\" + 0.004*\"applic\" + 0.003*\"vote\"\n",
      "Topic: 4 Word: 0.010*\"queri\" + 0.004*\"trajectori\" + 0.004*\"locat\" + 0.004*\"network\" + 0.004*\"data\" + 0.004*\"graph\" + 0.004*\"model\" + 0.004*\"user\" + 0.004*\"measur\" + 0.003*\"algorithm\"\n",
      "Topic: 5 Word: 0.006*\"data\" + 0.005*\"detect\" + 0.005*\"stream\" + 0.004*\"anomali\" + 0.004*\"network\" + 0.004*\"mobil\" + 0.003*\"user\" + 0.003*\"game\" + 0.003*\"graph\" + 0.003*\"pattern\"\n",
      "Topic: 6 Word: 0.004*\"program\" + 0.004*\"secur\" + 0.004*\"agent\" + 0.004*\"model\" + 0.004*\"process\" + 0.003*\"user\" + 0.003*\"method\" + 0.003*\"inform\" + 0.003*\"technolog\" + 0.003*\"data\"\n",
      "Topic: 7 Word: 0.009*\"cloud\" + 0.007*\"servic\" + 0.007*\"resourc\" + 0.005*\"schedul\" + 0.005*\"workflow\" + 0.004*\"comput\" + 0.004*\"model\" + 0.004*\"cluster\" + 0.004*\"algorithm\" + 0.004*\"user\"\n",
      "Topic: 8 Word: 0.007*\"cloud\" + 0.005*\"workflow\" + 0.005*\"data\" + 0.005*\"model\" + 0.005*\"solver\" + 0.004*\"string\" + 0.004*\"portfolio\" + 0.004*\"process\" + 0.004*\"comput\" + 0.004*\"algorithm\"\n",
      "Topic: 9 Word: 0.005*\"technolog\" + 0.004*\"user\" + 0.004*\"model\" + 0.004*\"social\" + 0.004*\"data\" + 0.004*\"cloud\" + 0.004*\"design\" + 0.004*\"anim\" + 0.004*\"process\" + 0.003*\"research\"\n",
      "Topic: 10 Word: 0.005*\"data\" + 0.004*\"network\" + 0.004*\"attack\" + 0.004*\"trajectori\" + 0.004*\"detect\" + 0.004*\"research\" + 0.004*\"process\" + 0.003*\"ethic\" + 0.003*\"cloud\" + 0.003*\"health\"\n",
      "Topic: 11 Word: 0.004*\"model\" + 0.004*\"data\" + 0.004*\"inform\" + 0.004*\"queri\" + 0.003*\"method\" + 0.003*\"semant\" + 0.003*\"technolog\" + 0.003*\"resourc\" + 0.003*\"social\" + 0.003*\"text\"\n",
      "Topic: 12 Word: 0.005*\"document\" + 0.005*\"queri\" + 0.004*\"data\" + 0.004*\"word\" + 0.004*\"collect\" + 0.004*\"compress\" + 0.004*\"user\" + 0.004*\"text\" + 0.003*\"model\" + 0.003*\"test\"\n",
      "Topic: 13 Word: 0.005*\"agent\" + 0.005*\"cod\" + 0.004*\"inform\" + 0.003*\"user\" + 0.003*\"servic\" + 0.003*\"model\" + 0.003*\"cloud\" + 0.003*\"search\" + 0.003*\"product\" + 0.003*\"algorithm\"\n",
      "Topic: 14 Word: 0.005*\"model\" + 0.004*\"featur\" + 0.004*\"select\" + 0.004*\"learn\" + 0.004*\"train\" + 0.003*\"gene\" + 0.003*\"function\" + 0.003*\"process\" + 0.003*\"feedback\" + 0.003*\"data\"\n"
     ]
    }
   ],
   "source": [
    "topics = list()\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1,10):\n",
    "    topics.append({\"topic_id\":idx,\"topic_ref\":topic})\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_id': 0, 'topic_ref': '0.007*\"sequenc\" + 0.004*\"cloud\" + 0.004*\"network\" + 0.004*\"data\" + 0.004*\"sensor\" + 0.004*\"model\" + 0.004*\"cluster\" + 0.003*\"word\" + 0.003*\"method\" + 0.003*\"protein\"'}\n",
      "{'topic_id': 1, 'topic_ref': '0.005*\"technolog\" + 0.005*\"research\" + 0.004*\"social\" + 0.004*\"health\" + 0.004*\"inform\" + 0.004*\"design\" + 0.004*\"data\" + 0.004*\"secur\" + 0.004*\"interact\" + 0.004*\"digit\"'}\n",
      "{'topic_id': 2, 'topic_ref': '0.006*\"data\" + 0.005*\"detect\" + 0.005*\"imag\" + 0.005*\"method\" + 0.004*\"cloud\" + 0.004*\"cluster\" + 0.004*\"model\" + 0.004*\"featur\" + 0.004*\"retin\" + 0.004*\"learn\"'}\n",
      "{'topic_id': 3, 'topic_ref': '0.006*\"cloud\" + 0.005*\"servic\" + 0.005*\"energi\" + 0.005*\"data\" + 0.005*\"network\" + 0.004*\"applic\" + 0.004*\"resourc\" + 0.004*\"mobil\" + 0.004*\"comput\" + 0.003*\"time\"'}\n",
      "{'topic_id': 4, 'topic_ref': '0.008*\"queri\" + 0.006*\"document\" + 0.005*\"model\" + 0.005*\"user\" + 0.004*\"process\" + 0.004*\"locat\" + 0.004*\"algorithm\" + 0.004*\"collect\" + 0.003*\"data\" + 0.003*\"retriev\"', 'topic_name': ['document databases', 'databases ranking algorithms']}\n",
      "{'topic_id': 5, 'topic_ref': '0.004*\"queri\" + 0.004*\"join\" + 0.003*\"data\" + 0.003*\"method\" + 0.003*\"model\" + 0.003*\"process\" + 0.003*\"sequenc\" + 0.003*\"inform\" + 0.003*\"object\" + 0.003*\"user\"'}\n",
      "{'topic_id': 6, 'topic_ref': '0.006*\"cod\" + 0.004*\"model\" + 0.004*\"secur\" + 0.004*\"data\" + 0.004*\"queri\" + 0.004*\"topic\" + 0.003*\"user\" + 0.003*\"attack\" + 0.003*\"servic\" + 0.003*\"cluster\"', 'topic_name': ['data security']}\n",
      "{'topic_id': 7, 'topic_ref': '0.006*\"agent\" + 0.006*\"interact\" + 0.005*\"game\" + 0.004*\"design\" + 0.004*\"technolog\" + 0.004*\"data\" + 0.004*\"cloud\" + 0.004*\"user\" + 0.003*\"research\" + 0.003*\"virtual\"'}\n",
      "{'topic_id': 8, 'topic_ref': '0.005*\"process\" + 0.004*\"model\" + 0.004*\"cluster\" + 0.004*\"graph\" + 0.004*\"algorithm\" + 0.003*\"technolog\" + 0.003*\"health\" + 0.003*\"user\" + 0.003*\"activ\" + 0.003*\"structur\"'}\n",
      "{'topic_id': 9, 'topic_ref': '0.006*\"cloud\" + 0.005*\"model\" + 0.004*\"agent\" + 0.004*\"servic\" + 0.003*\"algorithm\" + 0.003*\"data\" + 0.003*\"sequenc\" + 0.003*\"optim\" + 0.003*\"rule\" + 0.003*\"detect\"'}\n",
      "{'topic_id': 10, 'topic_ref': '0.006*\"cloud\" + 0.005*\"network\" + 0.004*\"user\" + 0.004*\"model\" + 0.004*\"data\" + 0.004*\"resourc\" + 0.004*\"energi\" + 0.004*\"problem\" + 0.004*\"servic\" + 0.003*\"document\"'}\n",
      "{'topic_id': 11, 'topic_ref': '0.004*\"data\" + 0.004*\"process\" + 0.004*\"queri\" + 0.004*\"method\" + 0.004*\"retriev\" + 0.003*\"model\" + 0.003*\"approach\" + 0.003*\"user\" + 0.003*\"inform\" + 0.003*\"document\"'}\n",
      "{'topic_id': 12, 'topic_ref': '0.005*\"data\" + 0.004*\"cloud\" + 0.004*\"process\" + 0.004*\"model\" + 0.004*\"resourc\" + 0.004*\"user\" + 0.004*\"workflow\" + 0.004*\"servic\" + 0.004*\"queri\" + 0.003*\"search\"'}\n",
      "{'topic_id': 13, 'topic_ref': '0.007*\"cloud\" + 0.005*\"model\" + 0.004*\"process\" + 0.004*\"servic\" + 0.004*\"resourc\" + 0.004*\"event\" + 0.003*\"data\" + 0.003*\"approach\" + 0.003*\"user\" + 0.003*\"compress\"'}\n",
      "{'topic_id': 14, 'topic_ref': '0.005*\"resourc\" + 0.005*\"cloud\" + 0.005*\"schedul\" + 0.004*\"execut\" + 0.004*\"network\" + 0.003*\"time\" + 0.003*\"method\" + 0.003*\"process\" + 0.003*\"comput\" + 0.003*\"perform\"'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopic(i):    \n",
    "    print(docText(documents,i))\n",
    "    for index, score in sorted(lda_model_tfidf[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "        topic_ref = lda_model_tfidf.print_topic(index);\n",
    "        print(score,topic_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supporting grid-based clinical trials in Scotland : A computational infrastructure to underpin complex clinical trials and medical population studies is highly desirable. This should allow access to a range of distributed clinical data sets; support the efficient processing and analysis of the data obtained; have security at its heart; and ensure that authorized individuals are able to see privileged data and no more. Each clinical trial has its own requirements on data sets and how they are used; hence a reusable and flexible framework offers many advantages. The MRC funded Virtual Organisations for Trials and Epidemiological Studies (VOTES) is a collaborative project involving several UK universities specifically to explore this space. This article presents the experiences of developing the Scottish component of this nationwide infrastructure, by the National e-Science Centre (NeSC) based at the University of Glasgow, and the issues inherent in accessing and using the clinical data sets in a flexible, dynamic and secure manner. © 2008 Sage Publications.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6, 10, 3]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def docText(documents,index):\n",
    "    text = documents[documents['index'] == index].values[0][0]\n",
    "    return text\n",
    "\n",
    "def getTopicId(i):    \n",
    "#     print(docText(documents,i))\n",
    "    topic_ids = list()\n",
    "    for index, score in sorted(lda_model_tfidf[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "        topic_ref = lda_model_tfidf.print_topic(index);\n",
    "        for topic in topics:\n",
    "            if topic_ref == topic[\"topic_ref\"]:\n",
    "                topic_ids.append(topic[\"topic_id\"])\n",
    "    return topic_ids\n",
    "\n",
    "# topic_table = {}\n",
    "# for i in range(0,len(documents)):\n",
    "#     topic_ids = getTopicId(i)\n",
    "#     for j in topic_ids:\n",
    "#         if j not in topic_table:\n",
    "#             topic_table[j] = list()\n",
    "#         else:\n",
    "#             topic_table[j].append(i)\n",
    "    \n",
    "\n",
    "getTopicId(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keyword = {}\n",
    "topic_keyword[0] = ['sensor','mutation','protein', 'sensor network', 'word sense','data', 'mining']\n",
    "topic_keyword[1] = ['clustering', 'interaction', 'community', 'exertion', 'gesture','process model','graph', 'measure']\n",
    "topic_keyword[2] = ['health','security', 'social', 'digital election', 'information', 'e-voting', 'internet technology', 'data privacy']\n",
    "topic_keyword[3] = ['cloud','public display','agent','game play','data center','energy consumption']\n",
    "topic_keyword[4] = ['query', 'document','location','data compression','search','model','tree','poi']\n",
    "topic_keyword[5] = ['complementary pair','arithmetic','correlation','optimal']\n",
    "topic_keyword[6] = ['code','security','clustering','coding','trajectory','privacy','attack','dependency','minimum redundancy','redundancy']\n",
    "topic_keyword[7] = ['game agent', 'mobile','technology', 'interaction', 'display', 'social', 'workshop', 'device','feedback', 'gesture']\n",
    "topic_keyword[8] = ['process', 'process model', 'graph', 'health', 'model', 'older', 'adoption', 'child', 'adult', 'older adult', 'clustering', 'protein', 'display', 'event', 'measure']\n",
    "topic_keyword[9] = ['agent', 'meta', 'xcp', 'max min', 'max', 'min', 'equilibrium', 'template', 'behaviour', 'pedestrian', 'fairness', 'meta model', 'groundness', 'fuzzy', 'fuzzy rule']\n",
    "topic_keyword[10] = ['resource', 'grid', 'cloud cluster', 'service', 'scheduling', 'optical', 'packet', 'energy', 'optical network', 'network', 'agent', 'allocation', 'core', 'size']\n",
    "topic_keyword[11] = ['word', 'string', 'scheme', 'deep', 'event', 'file', 'lexicon', 'query', 'forum', 'database', 'binary', 'index', 'retrieval', 'constraint', 'statement']\n",
    "topic_keyword[12] = ['document', 'compression', 'metric', 'query', 'word relevance', 'text retrieval', 'judgment', 'human', 'visual', 'collection', 'contrast']\n",
    "topic_keyword[13] = ['event', 'model', 'cloud', 'graph', 'behavioral', 'event log', 'program', 'resource', 'path', 'relation check']\n",
    "topic_keyword[14] = ['feedback', 'surgical', 'gene', 'feature', 'bone', 'selection', 'feature selection', 'classifier', 'xc', 'training', 'temporal bone', 'simulator']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPaperID(index):\n",
    "    id = documents[documents['index'] == index].values[0][2]\n",
    "    return id[:-1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments on \"On a novel unsupervised competitive learning algorithm for scalar quantization\" : This note proposes an efficient alternative to a recently proposed neural network for designing scalar quantizers. It also points out that the performance measure used is of limited applicability. © 1995 IEEE.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10, 1, 9]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTopicInfo(index):\n",
    "#     for index, score in sorted(lda_model_tfidf[bow_corpus[index]], key=lambda tup: -1*tup[1]):\n",
    "#         print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index)))\n",
    "    topic_ids = getTopicId(index)\n",
    "    if(len(topic_ids)>3):\n",
    "        topic_ids = topic_ids[0:2]\n",
    "    return topic_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# keywords_topic = {}\n",
    "# for i in range(0,15):\n",
    "#     keywords_topic[i] = Counter()\n",
    "\n",
    "import couchdb\n",
    "import csv\n",
    "\n",
    "couch=couchdb.Server(\"http://admin:password@localhost:5984\")\n",
    "try:\n",
    "    database=couch[\"paperinfo_scopus\"]\n",
    "except:\n",
    "    print(\"wrong db name\")\n",
    "    \n",
    "def addTopic(index):\n",
    "    id = getPaperID(index)\n",
    "    doc = database.get(id)\n",
    "#     todo : adding topic info:\n",
    "    doc[\"topic_ids\"] = getTopicInfo(index)\n",
    "    doc = database.save(doc)\n",
    "    \n",
    "for i in range(0,len(documents)):\n",
    "    addTopic(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topics:\n",
    "    topic[\"topic_name\"] = list()\n",
    "#     print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(15, 1), (42, 2), (48, 2), (67, 1), (84, 1), (96, 1), (103, 1), (143, 1), (160, 1), (193, 1), (199, 1), (204, 2), (262, 3), (312, 1), (317, 1), (348, 2), (349, 1), (414, 1), (435, 4), (436, 1), (456, 1), (483, 1), (545, 1), (678, 1), (864, 2), (918, 1), (939, 1), (940, 1), (968, 1), (970, 2), (1081, 1), (1210, 1), (1211, 1), (1212, 1), (1213, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(bow_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.38644173741340637\t \n",
      "Topic: 0.022*\"inform\" + 0.015*\"user\" + 0.011*\"data\" + 0.009*\"provid\" + 0.009*\"research\" + 0.008*\"evalu\" + 0.008*\"base\" + 0.007*\"network\" + 0.007*\"collect\" + 0.007*\"search\"\n",
      "\n",
      "Score: 0.3388003408908844\t \n",
      "Topic: 0.019*\"queri\" + 0.018*\"data\" + 0.016*\"process\" + 0.014*\"cloud\" + 0.009*\"propos\" + 0.008*\"time\" + 0.008*\"algorithm\" + 0.008*\"cost\" + 0.008*\"effici\" + 0.007*\"problem\"\n",
      "\n",
      "Score: 0.26342302560806274\t \n",
      "Topic: 0.010*\"propos\" + 0.009*\"agent\" + 0.009*\"method\" + 0.008*\"document\" + 0.008*\"similar\" + 0.008*\"game\" + 0.008*\"term\" + 0.007*\"effect\" + 0.007*\"model\" + 0.007*\"algorithm\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5246488451957703\t \n",
      "Topic: 0.004*\"program\" + 0.004*\"secur\" + 0.004*\"agent\" + 0.004*\"model\" + 0.004*\"process\" + 0.003*\"user\" + 0.003*\"method\" + 0.003*\"inform\" + 0.003*\"technolog\" + 0.003*\"data\"\n",
      "\n",
      "Score: 0.3234825134277344\t \n",
      "Topic: 0.009*\"cloud\" + 0.007*\"servic\" + 0.007*\"resourc\" + 0.005*\"schedul\" + 0.005*\"workflow\" + 0.004*\"comput\" + 0.004*\"model\" + 0.004*\"cluster\" + 0.004*\"algorithm\" + 0.004*\"user\"\n",
      "\n",
      "Score: 0.1436990201473236\t \n",
      "Topic: 0.005*\"technolog\" + 0.004*\"user\" + 0.004*\"model\" + 0.004*\"social\" + 0.004*\"data\" + 0.004*\"cloud\" + 0.004*\"design\" + 0.004*\"anim\" + 0.004*\"process\" + 0.003*\"research\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[3981]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test document has the highest probability to be part of the topic on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3733212351799011\t Topic: 0.006*\"mobile\" + 0.005*\"data\" + 0.004*\"social\" + 0.004*\"privacy\" + 0.003*\"research\"\n",
      "Score: 0.26307183504104614\t Topic: 0.006*\"cloud\" + 0.005*\"process\" + 0.005*\"data\" + 0.004*\"model\" + 0.004*\"document\"\n",
      "Score: 0.16387544572353363\t Topic: 0.006*\"game\" + 0.005*\"data\" + 0.004*\"design\" + 0.003*\"network\" + 0.003*\"mutations\"\n",
      "Score: 0.11993960291147232\t Topic: 0.005*\"cluster\" + 0.004*\"model\" + 0.004*\"data\" + 0.004*\"program\" + 0.003*\"network\"\n",
      "Score: 0.06462649255990982\t Topic: 0.004*\"data\" + 0.003*\"model\" + 0.003*\"network\" + 0.003*\"query\" + 0.003*\"process\"\n"
     ]
    }
   ],
   "source": [
    "# unseen_document = \"machine learning\"\n",
    "unseen_document = \"The use of randomness in the designing of the digital devices has been discussed. Qualities of randomness such as unpredictability, indeterminacy and unexpectedness have been used as a creative resource to generate innovative , output. Randomness is a creative tool to inspire and generate innovative outputs that is a means to an end. The growth of digital interactivity has been accompanied by a increasing amount of interactive that express certain qualities of randomness during use. An emergent approach toward randomness is to allow users to interact directly with the randomness. Shuffle listening, which is an alternative listening mode offered by digital music players, is a more sophisticated approach, whereby application of randomness has publicly captured by imagination of many people. Considerations, in determining where a random feature can be used, should include the types of content, the domain and contexts where these digital devices are used\"\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
